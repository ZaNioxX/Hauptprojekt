/home/sebastian/anaconda3/envs/pt/lib/python3.9/site-packages/lightning_fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
  rank_zero_warn(
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name  | Type                      | Params
----------------------------------------------------
0 | model | VisionEncoderDecoderModel | 201 M
----------------------------------------------------
201 M     Trainable params
0         Non-trainable params
201 M     Total params
807.461   Total estimated model params size (MB)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/sebastian/anaconda3/envs/pt/lib/python3.9/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )
  warnings.warn(
/home/sebastian/anaconda3/envs/pt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(
/home/sebastian/anaconda3/envs/pt/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
 Normed ED: 0.1812617702448211
 Normed ED: 0.4669909659485754
 Normed ED: 0.18546845124282982
 Normed ED: 0.36518324607329844
 Normed ED: 0.44024337244676226
 Normed ED: 0.2775297619047619
 Normed ED: 0.42012987012987013
 Normed ED: 0.30191854594412654
 Normed ED: 0.27964386887899634
 Normed ED: 0.8003367813326918
 Normed ED: 0.2895437262357414
 Normed ED: 0.2220026350461133
 Normed ED: 0.24486057398738043
 Normed ED: 0.20392953929539295
 Normed ED: 0.2754645560908465
 Normed ED: 0.006797583081570997
 Normed ED: 0.36777777777777776
 Normed ED: 0.0026613439787092482
 Normed ED: 0.015739769150052464
 Normed ED: 0.21361867704280155
 Normed ED: 0.4077314667515113
 Normed ED: 0.8015877003466814
 Normed ED: 0.41898332009531375
 Normed ED: 0.0005303632988597189
 Normed ED: 0.574757281553398
 Normed ED: 0.12931885488647582
 Normed ED: 0.010466222645099905
 Normed ED: 0.012138188608776844
 Normed ED: 0.01599247412982126
 Normed ED: 0.3526479750778816
 Normed ED: 0.5336594456091311
 Normed ED: 0.7217138103161398
 Normed ED: 0.022384174908901613
 Normed ED: 0.09739341281066882
 Normed ED: 0.4405132677214401
 Normed ED: 0.2533845590925723
 Normed ED: 0.23510750793091292
 Normed ED: 0.004329004329004329
 Normed ED: 0.13265879707700956
 Normed ED: 0.14749813293502614
 Normed ED: 0.09474768280123584
 Normed ED: 0.4842461122379986
 Normed ED: 0.03794244970715559
 Normed ED: 0.6340784479122733
 Normed ED: 0.6427661944930766
 Normed ED: 0.10644380910490434
 Normed ED: 0.5723222748815165
 Normed ED: 0.27675337504115904
 Normed ED: 0.13820522529344945
 Normed ED: 0.5955855190247507
 Normed ED: 0.6537505823885696
 Normed ED: 0.01029678982434888
 Normed ED: 0.2920508744038156
 Normed ED: 0.27369980250164583
 Normed ED: 0.5803444011036057
 Normed ED: 0.0036596523330283625
 Normed ED: 0.004591368227731864
 Normed ED: 0.0036330608537693005
 Normed ED: 0.007312614259597806
 Normed ED: 0.0027522935779816515
 Normed ED: 0.004646840148698885
 Normed ED: 0.001834862385321101
 Normed ED: 0.0018231540565177757
 Normed ED: 0.0018248175182481751
 Normed ED: 0.0018535681186283596
 Normed ED: 0.0027548209366391185
 Normed ED: 0.0036663611365719525
 Normed ED: 0.004591368227731864
 Normed ED: 0.0018484288354898336
 Normed ED: 0.0018484288354898336
 Normed ED: 0.004604051565377533
 Normed ED: 0.001841620626151013
 Normed ED: 0.0018181818181818182
 Normed ED: 0.0018231540565177757
 Normed ED: 0.0018315018315018315
 Normed ED: 0.0018433179723502304
 Normed ED: 0.003663003663003663
 Normed ED: 0.004549590536851683
 Normed ED: 0.0027522935779816515
 Normed ED: 0.0018433179723502304
 Normed ED: 0.0018501387604070306
 Normed ED: 0.003663003663003663
 Normed ED: 0.0018365472910927456
 Normed ED: 0.005504587155963303
 Normed ED: 0.004604051565377533
 Normed ED: 0.0018281535648994515
 Normed ED: 0.0018148820326678765
 Normed ED: 0.0036529680365296802
 Normed ED: 0.0027472527472527475
 Normed ED: 0.0036596523330283625
 Normed ED: 0.0018198362147406734
 Normed ED: 0.002749770852428964
 Normed ED: 0.00273224043715847
 Normed ED: 0.0027624309392265192
 Normed ED: 0.028985507246376812
 Normed ED: 0.003751563151313047
 Normed ED: 0.002956194571351787
 Normed ED: 0.0024096385542168677
 Normed ED: 0.0033516878142207324
 Normed ED: 0.09124324324324325
 Normed ED: 0.005724615848147032
 Normed ED: 0.0049833887043189366
 Normed ED: 0.004791056694170881
 Normed ED: 0.20824421543846006
 Normed ED: 0.029875323453305104
 Normed ED: 0.2053069920588805
 Normed ED: 0.3672787979966611
 Normed ED: 0.12345265975242556
 Normed ED: 0.018615751789976133
 Normed ED: 0.018223234624145785
 Normed ED: 0.017430845017051912
 Normed ED: 0.1449639159083778
 Normed ED: 0.012606366214938543
 Normed ED: 0.014345035862589657
 Normed ED: 0.2994518044769301
 Normed ED: 0.009882174078297225
 Normed ED: 0.01930862659607599
 Normed ED: 0.0509433962264151
 Normed ED: 0.018425983760149905
 Normed ED: 0.007563819728963126
 Normed ED: 0.010658307210031349
 Normed ED: 0.01836290071584189
 Normed ED: 0.012013455069678039
 Normed ED: 0.02041457286432161
 Normed ED: 0.015583428354237932
 Normed ED: 0.12271448663853728
 Normed ED: 0.19489912447658927
 Normed ED: 0.6937882764654418
 Normed ED: 0.006191950464396285
 Normed ED: 0.13766842413591096
 Normed ED: 0.8427897723085992
 Normed ED: 0.5777389022523508
 Normed ED: 0.04816955684007707
 Normed ED: 0.0
 Normed ED: 0.20330459770114942
 Normed ED: 0.0006684491978609625
 Normed ED: 0.013765748950069996
 Normed ED: 0.0
 Normed ED: 0.0190826916638768
 Normed ED: 0.002107234839616015
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.19157706093189963
 Normed ED: 0.000591016548463357
 Normed ED: 0.000591715976331361
 Normed ED: 0.006043956043956044
 Normed ED: 0.06718241042345277
 Normed ED: 0.08230285830123657
 Normed ED: 0.05133928571428571
 Normed ED: 0.028738317757009347
 Normed ED: 0.0770478507704785
 Normed ED: 0.0006697923643670462
 Normed ED: 0.0
 Normed ED: 0.009478672985781991
 Normed ED: 0.007452387524151256
 Normed ED: 0.00631911532385466
 Normed ED: 0.004156275976724855
 Normed ED: 0.008004416229643941
 Normed ED: 0.004136789851075565
 Normed ED: 0.0063641394576646375
 Normed ED: 0.06437414030261349
 Normed ED: 0.043717999212288305
 Normed ED: 0.041635124905374715
 Normed ED: 0.004375497215592681
 Normed ED: 0.00498200941046222
 Normed ED: 0.00637649015802606
 Normed ED: 0.0027755749405233942
 Normed ED: 0.012633241215949467
 Normed ED: 0.004715672676837725
 Normed ED: 0.0047430830039525695
 Normed ED: 0.006075669704501519
 Normed ED: 0.004972032318210068
 Normed ED: 0.0031201248049922
 Normed ED: 0.005793742757821553
 Normed ED: 0.005607476635514018
 Normed ED: 0.010852713178294573
 Normed ED: 0.1435
 Normed ED: 0.6930283824745109
 Normed ED: 0.0022530980097634247
 Normed ED: 0.0533044420368364
 Normed ED: 0.18169877408056043
 Normed ED: 0.21235365361324182
 Normed ED: 0.46541049798115747
 Normed ED: 0.11616038882138517
 Normed ED: 0.02147239263803681
 Normed ED: 0.028034066713981547
 Normed ED: 0.5646585447636079
 Normed ED: 0.5857287449392713
 Normed ED: 0.39294636296840557
 Normed ED: 0.12914485165794065
 Normed ED: 0.5869650185655657
 Normed ED: 0.0614923134608174
 Normed ED: 0.1503388786198398
 Normed ED: 0.13781063873485316
 Normed ED: 0.14328210213187903
 Normed ED: 0.11368715083798883
 Normed ED: 0.003069053708439898
 Normed ED: 0.13201246660730187
 Normed ED: 0.16751269035532995
 Normed ED: 0.25321386832878845
 Normed ED: 0.08432304038004751
 Normed ED: 0.47183278103492227
 Normed ED: 0.1293274970155193
 Normed ED: 0.37719173200285105
 Normed ED: 0.0704830053667263
 Normed ED: 0.06489563567362429
 Normed ED: 0.82204461118134
 Normed ED: 0.6262599930483143
 Normed ED: 0.8366162372678251
 Normed ED: 0.8687236604455147
 Normed ED: 0.3036883833160979
 Normed ED: 0.13433250069386624
 Normed ED: 0.14178765880217786
 Normed ED: 0.1686010995723885
 Normed ED: 0.31968295904887717
 Normed ED: 0.12823252831801668
 Normed ED: 0.24324324324324326
 Normed ED: 0.16405919661733614
 Normed ED: 0.047512437810945274
 Normed ED: 0.023343373493975902
 Normed ED: 0.0022875816993464053
 Normed ED: 0.09638109305760709
 Normed ED: 0.0054926192928252664
 Normed ED: 0.8453089244851258
 Normed ED: 0.4778176763581605
 Normed ED: 0.5056524944703858
 Normed ED: 0.5188760188760189
 Normed ED: 0.23133672931670474
 Normed ED: 0.11533505154639176
 Normed ED: 0.12100780695528744
 Normed ED: 0.012782401902497027
 Normed ED: 0.0032679738562091504
 Normed ED: 0.003604325190228274
 Normed ED: 0.7268202412098781
 Normed ED: 0.34872944693572494
 Normed ED: 0.7314229003845677
 Normed ED: 0.788386403594452
 Normed ED: 0.7251597118390648
 Normed ED: 0.00870223231176693
 Normed ED: 0.005002779321845469
 Normed ED: 0.0015180265654648956
 Normed ED: 0.4340590764110387
 Normed ED: 0.2734208658623137
 Normed ED: 0.27882227740333454
 Normed ED: 0.05914893617021277
 Normed ED: 0.499431947284708
 Normed ED: 0.10151668351870577
 Normed ED: 0.2656198347107438
 Normed ED: 0.04967261232783924
 Normed ED: 0.22629969418960244
Pushing model to the hub, epoch 0
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/sebastian/anaconda3/envs/pt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(
 Normed ED: 0.2618421052631579
 Normed ED: 0.2497339482085846
 Normed ED: 0.014340344168260038
 Normed ED: 0.14336917562724014
 Normed ED: 0.4495147037519919
 Normed ED: 0.16576487948844074
 Normed ED: 0.2649350649350649
 Normed ED: 0.284079434533827
 Normed ED: 0.19668150546337515
 Normed ED: 0.7845965770171149
 Normed ED: 0.7606463878326997
 Normed ED: 0.4176548089591568
 Normed ED: 0.27498473437818033
 Normed ED: 0.2302158273381295
 Normed ED: 0.23589125946317963
 Normed ED: 0.004531722054380665
 Normed ED: 0.35777777777777775
 Normed ED: 0.0006653359946773121
 Normed ED: 0.011046817464492372
 Normed ED: 0.017120622568093387
 Normed ED: 0.42857142857142855
 Normed ED: 0.7989750288901171
 Normed ED: 0.41739475774424145
 Normed ED: 0.0007955449482895784
 Normed ED: 0.574368932038835
 Normed ED: 0.09272727272727273
 Normed ED: 0.009514747859181731
 Normed ED: 0.011204481792717087
 Normed ED: 0.011288805268109126
 Normed ED: 0.012287334593572778
 Normed ED: 0.5201490798975076
 Normed ED: 0.7169301164725458
 Normed ED: 0.01353461738677772
 Normed ED: 0.09476661951909476
 Normed ED: 0.43503176778372993
 Normed ED: 0.24533479692645443
 Normed ED: 0.24709199859005992
 Normed ED: 0.002405002405002405
 Normed ED: 0.007305422871593144
 Normed ED: 0.08065720687079911
 Normed ED: 0.0033407572383073497
 Normed ED: 0.450709939148073
 Normed ED: 0.006873727087576375
 Normed ED: 0.6237030788696752
 Normed ED: 0.6436415724972147
 Normed ED: 0.01084949215143121
 Normed ED: 0.5727014218009479
 Normed ED: 0.2737899242673691
 Normed ED: 0.14691404770920105
 Normed ED: 0.5942002216475804
 Normed ED: 0.6522751980121136
 Normed ED: 0.012070006035003017
 Normed ED: 0.28553259141494436
 Normed ED: 0.26003949967083606
 Normed ED: 0.5755874797830843
 Normed ED: 0.0027447392497712718
 Normed ED: 0.003676470588235294
 Normed ED: 0.0018198362147406734
 Normed ED: 0.002742230347349177
 Normed ED: 0.0018365472910927456
 Normed ED: 0.0018604651162790699
 Normed ED: 0.001834862385321101
 Normed ED: 0.0018231540565177757
 Normed ED: 0.0018248175182481751
 Normed ED: 0.0018535681186283596
 Normed ED: 0.0018365472910927456
 Normed ED: 0.005494505494505495
 Normed ED: 0.003676470588235294
 Normed ED: 0.0018484288354898336
 Normed ED: 0.0018484288354898336
 Normed ED: 0.0018433179723502304
 Normed ED: 0.001841620626151013
 Normed ED: 0.0018181818181818182
 Normed ED: 0.0018231540565177757
 Normed ED: 0.0018315018315018315
 Normed ED: 0.0018433179723502304
 Normed ED: 0.003663003663003663
 Normed ED: 0.006369426751592357
 Normed ED: 0.0027548209366391185
 Normed ED: 0.0018433179723502304
 Normed ED: 0.0018501387604070306
 Normed ED: 0.003663003663003663
 Normed ED: 0.0027548209366391185
 Normed ED: 0.0045871559633027525
 Normed ED: 0.003683241252302026
 Normed ED: 0.0018281535648994515
 Normed ED: 0.0018148820326678765
 Normed ED: 0.0018281535648994515
 Normed ED: 0.0018331805682859762
 Normed ED: 0.0018315018315018315
 Normed ED: 0.0018198362147406734
 Normed ED: 0.002749770852428964
 Normed ED: 0.0018231540565177757
 Normed ED: 0.004604051565377533
 Normed ED: 0.004732739420935412
 Normed ED: 0.0029190992493744786
 Normed ED: 0.0018812147272238647
 Normed ED: 0.002108433734939759
 Normed ED: 0.0026347305389221557
 Normed ED: 0.10724324324324325
 Normed ED: 0.006628502561012353
 Normed ED: 0.004152823920265781
 Normed ED: 0.003459286854709952
 Normed ED: 0.1977445070970251
 Normed ED: 0.02846389084921195
 Normed ED: 0.20646910710827038
 Normed ED: 0.31819699499165277
 Normed ED: 0.10672323759791123
 Normed ED: 0.014354066985645933
 Normed ED: 0.012148823082763858
 Normed ED: 0.010238907849829351
 Normed ED: 0.00815814245371823
 Normed ED: 0.041601008509297197
 Normed ED: 0.016610041525103814
 Normed ED: 0.01606805293005671
 Normed ED: 0.0068415051311288486
 Normed ED: 0.006847183317771553
 Normed ED: 0.04867924528301887
 Normed ED: 0.008109794135995009
 Normed ED: 0.004408060453400504
 Normed ED: 0.010031347962382446
 Normed ED: 0.006285355122564425
 Normed ED: 0.009117082533589251
 Normed ED: 0.013211701793016672
 Normed ED: 0.014063093880653743
 Normed ED: 0.005274261603375527
 Normed ED: 0.10063897763578275
 Normed ED: 0.6933110633898035
 Normed ED: 0.005413766434648105
 Normed ED: 0.13376293692638155
 Normed ED: 0.8426792425023948
 Normed ED: 0.535425322545375
 Normed ED: 0.000275178866263071
 Normed ED: 0.000591715976331361
 Normed ED: 0.1995330459770115
 Normed ED: 0.0
 Normed ED: 0.0006999533364442371
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.00023413720440177945
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.1992831541218638
 Normed ED: 0.000591016548463357
 Normed ED: 0.0005920663114268798
 Normed ED: 0.0021971985718209283
 Normed ED: 0.06758957654723127
 Normed ED: 0.07703223190756132
 Normed ED: 0.07467532467532467
 Normed ED: 0.00046728971962616824
 Normed ED: 0.08231954582319546
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.0015791551519936833
 Normed ED: 0.004965517241379311
 Normed ED: 0.0039494470774091624
 Normed ED: 0.003048780487804878
 Normed ED: 0.005247169290251312
 Normed ED: 0.003309431880860452
 Normed ED: 0.004702627939142462
 Normed ED: 0.0041242782513060215
 Normed ED: 0.0023631350925561244
 Normed ED: 0.003933910306845004
 Normed ED: 0.003979307600477517
 Normed ED: 0.005812344312205923
 Normed ED: 0.006099251455503188
 Normed ED: 0.0023790642347343376
 Normed ED: 0.008685353335965259
 Normed ED: 0.004438280166435506
 Normed ED: 0.002372479240806643
 Normed ED: 0.009389671361502348
 Normed ED: 0.00373366521468575
 Normed ED: 0.0046801872074883
 Normed ED: 0.002317497103128621
 Normed ED: 0.003738317757009346
 Normed ED: 0.006968641114982578
 Normed ED: 0.005
 Normed ED: 0.2993593476994758
 Normed ED: 0.0030041306796845663
 Normed ED: 0.05308775731310943
 Normed ED: 0.047285464098073555
 Normed ED: 0.059345983044004846
 Normed ED: 0.4547779273216689
 Normed ED: 0.015795868772782502
 Normed ED: 0.015775635407537247
 Normed ED: 0.0046132008516678496
 Normed ED: 0.5285570582318095
 Normed ED: 0.5746963562753037
 Normed ED: 0.35488611315209406
 Normed ED: 0.0028892455858747996
 Normed ED: 0.5840336134453782
 Normed ED: 0.0026646364674533687
 Normed ED: 0.003694581280788177
 Normed ED: 0.1589648798521257
 Normed ED: 0.17609444171175603
 Normed ED: 0.053806525472238124
 Normed ED: 0.0020460358056265983
 Normed ED: 0.051869991095280496
 Normed ED: 0.2827906976744186
 Normed ED: 0.31573821581612777
 Normed ED: 0.03483768804433888
 Normed ED: 0.4705582462401224
 Normed ED: 0.14464783127735775
 Normed ED: 0.3940128296507484
 Normed ED: 0.0032200357781753132
 Normed ED: 0.003795066413662239
 Normed ED: 0.8226155038127472
 Normed ED: 0.625738616614529
 Normed ED: 0.8358672857998801
 Normed ED: 0.8687236604455147
 Normed ED: 0.32764563943467767
 Normed ED: 0.13374233128834356
 Normed ED: 0.13611615245009073
 Normed ED: 0.3324555628703094
 Normed ED: 0.036988110964332896
 Normed ED: 0.06069673007052789
 Normed ED: 0.2844020491079315
 Normed ED: 0.013102282333051564
 Normed ED: 0.0037641154328732747
 Normed ED: 0.005271084337349397
 Normed ED: 0.048064918851435705
 Normed ED: 0.009681323114158934
 Normed ED: 0.004806041881222108
 Normed ED: 0.8184439359267734
 Normed ED: 0.4853469245916831
 Normed ED: 0.46940280167117227
 Normed ED: 0.5212355212355212
 Normed ED: 0.23713332162304585
 Normed ED: 0.01610824742268041
 Normed ED: 0.005952380952380952
 Normed ED: 0.00267538644470868
 Normed ED: 0.00267379679144385
 Normed ED: 0.002003205128205128
 Normed ED: 0.7245230042754132
 Normed ED: 0.34723467862481316
 Normed ED: 0.7314229003845677
 Normed ED: 0.7882398906036335
 Normed ED: 0.724412124507272
 Normed ED: 0.07453651153991676
 Normed ED: 0.11012235817575083
 Normed ED: 0.06907020872865276
 Normed ED: 0.4373873249202607
 Normed ED: 0.2755500354861604
 Normed ED: 0.2681802057467187
 Normed ED: 0.012917594654788419
 Normed ED: 0.49693251533742333
 Normed ED: 0.09646107178968655
 Normed ED: 0.31074380165289256
 Normed ED: 0.0018115942028985507
 Normed ED: 0.12538226299694188
Pushing model to the hub, epoch 1
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/sebastian/anaconda3/envs/pt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 Normed ED: 0.5398659579385255
 Normed ED: 0.8148245845423371
 Normed ED: 0.43986254295532645
 Normed ED: 0.14209591474245115
 Normed ED: 0.4474866000289729
 Normed ED: 0.0886964661830096
 Normed ED: 0.287012987012987
 Normed ED: 0.28929653315382026
 Normed ED: 0.12670349907918968
 Normed ED: 0.3161764705882353
 Normed ED: 0.8986692015209126
 Normed ED: 0.18050065876152832
 Normed ED: 0.919397516792184
 Normed ED: 0.15089163237311384
 Normed ED: 0.22677219545767377
 Normed ED: 0.004531722054380665
 Normed ED: 0.355
 Normed ED: 0.000998003992015968
 Normed ED: 0.004208311415044713
 Normed ED: 0.006614785992217899
 Normed ED: 0.3650970410435889
 Normed ED: 0.7966638195246948
 Normed ED: 0.41540905480540113
 Normed ED: 0.0005303632988597189
 Normed ED: 0.5742718446601942
 Normed ED: 0.002303389272787101
 Normed ED: 0.008563273073263558
 Normed ED: 0.008403361344537815
 Normed ED: 0.010338345864661654
 Normed ED: 0.012287334593572778
 Normed ED: 0.5096668996040066
 Normed ED: 0.7112451469772602
 Normed ED: 0.014055179593961478
 Normed ED: 0.08628005657708628
 Normed ED: 0.42780615422947554
 Normed ED: 0.24094401756311745
 Normed ED: 0.2229467747620726
 Normed ED: 0.001924001924001924
 Normed ED: 0.0016863406408094434
 Normed ED: 0.0022404779686333084
 Normed ED: 0.0033407572383073497
 Normed ED: 0.4438133874239351
 Normed ED: 0.0053435114503816794
 Normed ED: 0.6235343736819907
 Normed ED: 0.6379914053795958
 Normed ED: 0.010407030527289547
 Normed ED: 0.5710900473933649
 Normed ED: 0.2810339150477445
 Normed ED: 0.14672472548277168
 Normed ED: 0.5950314000738826
 Normed ED: 0.652974064295698
 Normed ED: 0.0030358227079538553
 Normed ED: 0.2874403815580286
 Normed ED: 0.2641540487162607
 Normed ED: 0.5716868043002569
 Normed ED: 0.004574565416285453
 Normed ED: 0.001838235294117647
 Normed ED: 0.0018198362147406734
 Normed ED: 0.003656307129798903
 Normed ED: 0.0036730945821854912
 Normed ED: 0.0018604651162790699
 Normed ED: 0.001834862385321101
 Normed ED: 0.0018231540565177757
 Normed ED: 0.0036496350364963502
 Normed ED: 0.0018535681186283596
 Normed ED: 0.0018365472910927456
 Normed ED: 0.004578754578754579
 Normed ED: 0.007352941176470588
 Normed ED: 0.0018484288354898336
 Normed ED: 0.0046210720887245845
 Normed ED: 0.0018433179723502304
 Normed ED: 0.001841620626151013
 Normed ED: 0.0018181818181818182
 Normed ED: 0.0027347310847766638
 Normed ED: 0.0018315018315018315
 Normed ED: 0.0018433179723502304
 Normed ED: 0.0036596523330283625
 Normed ED: 0.003639672429481347
 Normed ED: 0.0018365472910927456
 Normed ED: 0.0018433179723502304
 Normed ED: 0.0018501387604070306
 Normed ED: 0.0018315018315018315
 Normed ED: 0.0027522935779816515
 Normed ED: 0.0027522935779816515
 Normed ED: 0.003683241252302026
 Normed ED: 0.0018281535648994515
 Normed ED: 0.0018148820326678765
 Normed ED: 0.0018281535648994515
 Normed ED: 0.0018331805682859762
 Normed ED: 0.0018315018315018315
 Normed ED: 0.0018198362147406734
 Normed ED: 0.0018331805682859762
 Normed ED: 0.0036463081130355514
 Normed ED: 0.004604051565377533
 Normed ED: 0.0013935340022296545
 Normed ED: 0.003336113427856547
 Normed ED: 0.0026874496103198066
 Normed ED: 0.002108433734939759
 Normed ED: 0.0016766467065868263
 Normed ED: 0.08389189189189189
 Normed ED: 0.0027116601385959627
 Normed ED: 0.0033222591362126247
 Normed ED: 0.0023955283470854403
 Normed ED: 0.19346684814310713
 Normed ED: 0.029169607151258527
 Normed ED: 0.20491962037575054
 Normed ED: 0.02003338898163606
 Normed ED: 0.03758169934640523
 Normed ED: 0.011483253588516746
 Normed ED: 0.01442672741078208
 Normed ED: 0.006825938566552901
 Normed ED: 0.0059561128526645765
 Normed ED: 0.009766855702583491
 Normed ED: 0.010192525481313703
 Normed ED: 0.011657214870825458
 Normed ED: 0.006081337894336754
 Normed ED: 0.007160647571606476
 Normed ED: 0.008679245283018867
 Normed ED: 0.008107265357031494
 Normed ED: 0.002205419029615627
 Normed ED: 0.1871473354231975
 Normed ED: 0.00565149136577708
 Normed ED: 0.008641382621219395
 Normed ED: 0.010354565422026984
 Normed ED: 0.008361839604713038
 Normed ED: 0.005274261603375527
 Normed ED: 0.01870274572224433
 Normed ED: 0.6709615843474112
 Normed ED: 0.006187161639597835
 Normed ED: 0.13317711384495215
 Normed ED: 0.8420529069339032
 Normed ED: 0.5672425103870544
 Normed ED: 0.000275178866263071
 Normed ED: 0.0
 Normed ED: 0.2002514367816092
 Normed ED: 0.0
 Normed ED: 0.0011665888940737283
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.0004682744088035589
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.19874551971326165
 Normed ED: 0.000591016548463357
 Normed ED: 0.0005920663114268798
 Normed ED: 0.0008241758241758242
 Normed ED: 0.06697882736156352
 Normed ED: 0.08676261909588485
 Normed ED: 0.07548701298701299
 Normed ED: 0.0
 Normed ED: 0.08231954582319546
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.002368732727990525
 Normed ED: 0.0038642009384487995
 Normed ED: 0.0035545023696682463
 Normed ED: 0.0019401330376940134
 Normed ED: 0.0030378348522507597
 Normed ED: 0.003861003861003861
 Normed ED: 0.004979253112033195
 Normed ED: 0.03493810178817056
 Normed ED: 0.0035419126328217238
 Normed ED: 0.0019677292404565133
 Normed ED: 0.0035828025477707007
 Normed ED: 0.004427227448810182
 Normed ED: 0.003049625727751594
 Normed ED: 0.002776675922253074
 Normed ED: 0.008290564547966837
 Normed ED: 0.0027739251040221915
 Normed ED: 0.0039510075069142635
 Normed ED: 0.005518763796909493
 Normed ED: 0.00373366521468575
 Normed ED: 0.00234009360374415
 Normed ED: 0.005793742757821553
 Normed ED: 0.0043586550435865505
 Normed ED: 0.00890092879256966
 Normed ED: 0.0025
 Normed ED: 0.016923076923076923
 Normed ED: 0.0026286143447239955
 Normed ED: 0.050920910075839654
 Normed ED: 0.008318739054290718
 Normed ED: 0.002825999192571659
 Normed ED: 0.4514131897711978
 Normed ED: 0.014094775212636695
 Normed ED: 0.011831726555652936
 Normed ED: 0.0031926214969847464
 Normed ED: 0.5276629037666257
 Normed ED: 0.5738866396761133
 Normed ED: 0.3573842762674504
 Normed ED: 0.0016066838046272494
 Normed ED: 0.586281024037522
 Normed ED: 0.002663622526636225
 Normed ED: 0.0030807147258163892
 Normed ED: 0.10987882522078456
 Normed ED: 0.0046559751681324365
 Normed ED: 0.03302803219539273
 Normed ED: 0.0025575447570332483
 Normed ED: 0.05632235084594835
 Normed ED: 0.07049763033175356
 Normed ED: 0.2596416049863654
 Normed ED: 0.001979414093428345
 Normed ED: 0.4714504205964823
 Normed ED: 0.1820533227218464
 Normed ED: 0.37177476835352813
 Normed ED: 0.0025044722719141325
 Normed ED: 0.00683111954459203
 Normed ED: 0.821963055091139
 Normed ED: 0.625564824469934
 Normed ED: 0.8357923906530856
 Normed ED: 0.868994581577363
 Normed ED: 0.27404343329886244
 Normed ED: 0.1588372696600052
 Normed ED: 0.14632486388384755
 Normed ED: 0.10153256704980843
 Normed ED: 0.04227212681638045
 Normed ED: 0.10023509296858303
 Normed ED: 0.24253665430136018
 Normed ED: 0.009725158562367865
 Normed ED: 0.003262233375156838
 Normed ED: 0.0025100401606425703
 Normed ED: 0.00196078431372549
 Normed ED: 0.007255139056831923
 Normed ED: 0.0037761757638173706
 Normed ED: 0.814187643020595
 Normed ED: 0.4856944283563072
 Normed ED: 0.46153846153846156
 Normed ED: 0.5176962676962676
 Normed ED: 0.2287019146320042
 Normed ED: 0.00902061855670103
 Normed ED: 0.004767580452920143
 Normed ED: 0.06391200951248514
 Normed ED: 0.06417112299465241
 Normed ED: 0.001201923076923077
 Normed ED: 0.7265649926616042
 Normed ED: 0.34723467862481316
 Normed ED: 0.7314229003845677
 Normed ED: 0.7882398906036335
 Normed ED: 0.7209460377871415
 Normed ED: 0.004161937192584185
 Normed ED: 0.002224694104560623
 Normed ED: 0.0015180265654648956
 Normed ED: 0.4337817223686035
 Normed ED: 0.27288857345635203
 Normed ED: 0.2798864845689961
 Normed ED: 0.0017977528089887641
 Normed ED: 0.49761417859577367
 Normed ED: 0.09504550050556117
 Normed ED: 0.2674380165289256
 Normed ED: 0.0006793478260869565
 Normed ED: 0.006619144602851324
Pushing model to the hub, epoch 2
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/sebastian/anaconda3/envs/pt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 Normed ED: 0.29403606102635227
 Normed ED: 0.959914863426747
 Normed ED: 0.06883365200764818
 Normed ED: 0.0718562874251497
 Normed ED: 0.455743879472693
 Normed ED: 0.15742690058479533
 Normed ED: 0.3226744186046512
 Normed ED: 0.3074722315718613
 Normed ED: 0.12106918238993711
 Normed ED: 0.20756172839506173
 Normed ED: 0.3361216730038023
 Normed ED: 0.7160737812911726
 Normed ED: 0.2084266232444535
 Normed ED: 0.0014925373134328358
 Normed ED: 0.262388162422574
 Normed ED: 0.004528301886792453
 Normed ED: 0.35125
 Normed ED: 0.001996007984031936
 Normed ED: 0.003156233561283535
 Normed ED: 0.011673151750972763
 Normed ED: 0.3773464842507159
 Normed ED: 0.7963121137516957
 Normed ED: 0.4144823934339423
 Normed ED: 0.0005303632988597189
 Normed ED: 0.5740776699029126
 Normed ED: 0.002303389272787101
 Normed ED: 0.008563273073263558
 Normed ED: 0.01027077497665733
 Normed ED: 0.008466603951081843
 Normed ED: 0.008514664143803218
 Normed ED: 0.5101327742837177
 Normed ED: 0.7101358846367166
 Normed ED: 0.019260801665799063
 Normed ED: 0.083855324307941
 Normed ED: 0.43278933599103026
 Normed ED: 0.2530186608122942
 Normed ED: 0.22329925978145929
 Normed ED: 0.003367003367003367
 Normed ED: 0.0016863406408094434
 Normed ED: 0.041448842419716206
 Normed ED: 0.0033407572383073497
 Normed ED: 0.4455713319810683
 Normed ED: 0.008148714031066972
 Normed ED: 0.623618726275833
 Normed ED: 0.6367181282826675
 Normed ED: 0.009481961147086031
 Normed ED: 0.569478672985782
 Normed ED: 0.2728021073427725
 Normed ED: 0.16054524801211661
 Normed ED: 0.5985408200960473
 Normed ED: 0.6525858052492624
 Normed ED: 0.012650602409638554
 Normed ED: 0.28696343402225755
 Normed ED: 0.2682685977616853
 Normed ED: 0.5757777566359052
 Normed ED: 0.0036596523330283625
 Normed ED: 0.001838235294117647
 Normed ED: 0.0018198362147406734
 Normed ED: 0.003656307129798903
 Normed ED: 0.0018365472910927456
 Normed ED: 0.0018604651162790699
 Normed ED: 0.001834862385321101
 Normed ED: 0.0018231540565177757
 Normed ED: 0.0018248175182481751
 Normed ED: 0.0018535681186283596
 Normed ED: 0.0018365472910927456
 Normed ED: 0.002749770852428964
 Normed ED: 0.003676470588235294
 Normed ED: 0.0018484288354898336
 Normed ED: 0.0046210720887245845
 Normed ED: 0.0018433179723502304
 Normed ED: 0.001841620626151013
 Normed ED: 0.0018181818181818182
 Normed ED: 0.0018231540565177757
 Normed ED: 0.0018315018315018315
 Normed ED: 0.0027649769585253456
 Normed ED: 0.004574565416285453
 Normed ED: 0.00272975432211101
 Normed ED: 0.0018365472910927456
 Normed ED: 0.0018433179723502304
 Normed ED: 0.0018501387604070306
 Normed ED: 0.0018315018315018315
 Normed ED: 0.0018365472910927456
 Normed ED: 0.003669724770642202
 Normed ED: 0.003683241252302026
 Normed ED: 0.0018281535648994515
 Normed ED: 0.0018148820326678765
 Normed ED: 0.002742230347349177
 Normed ED: 0.0018331805682859762
 Normed ED: 0.0027472527472527475
 Normed ED: 0.0018198362147406734
 Normed ED: 0.0018331805682859762
 Normed ED: 0.0027347310847766638
 Normed ED: 0.0027624309392265192
 Normed ED: 0.0016722408026755853
 Normed ED: 0.0029190992493744786
 Normed ED: 0.0018812147272238647
 Normed ED: 0.002108433734939759
 Normed ED: 0.0021551724137931034
 Normed ED: 0.08562162162162162
 Normed ED: 0.004218137993371497
 Normed ED: 0.0029069767441860465
 Normed ED: 0.002661698163428267
 Normed ED: 0.19463348240326658
 Normed ED: 0.027993413314514232
 Normed ED: 0.20491962037575054
 Normed ED: 0.01335559265442404
 Normed ED: 0.005352960856473737
 Normed ED: 0.005741626794258373
 Normed ED: 0.009111617312072893
 Normed ED: 0.007930513595166163
 Normed ED: 0.00439146800501882
 Normed ED: 0.007248660573589662
 Normed ED: 0.010947527368818422
 Normed ED: 0.010681746779767515
 Normed ED: 0.010642341315089319
 Normed ED: 0.28589224540641545
 Normed ED: 0.013187641296156745
 Normed ED: 0.0046801872074883
 Normed ED: 0.004727387330601954
 Normed ED: 0.006269592476489028
 Normed ED: 0.005026704366949419
 Normed ED: 0.0067243035542747355
 Normed ED: 0.007530593034201443
 Normed ED: 0.010262257696693273
 Normed ED: 0.004219409282700422
 Normed ED: 0.00878594249201278
 Normed ED: 0.6839258729022508
 Normed ED: 0.02786377708978328
 Normed ED: 0.13376293692638155
 Normed ED: 0.8426792425023948
 Normed ED: 0.5326918871637875
 Normed ED: 0.0002752546105147261
 Normed ED: 0.0
 Normed ED: 0.20079022988505746
 Normed ED: 0.0
 Normed ED: 0.0006999533364442371
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.0007024116132053383
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.1992831541218638
 Normed ED: 0.000591016548463357
 Normed ED: 0.000591715976331361
 Normed ED: 0.0008241758241758242
 Normed ED: 0.07125407166123779
 Normed ED: 0.08534360429758768
 Normed ED: 0.07589285714285714
 Normed ED: 0.0
 Normed ED: 0.08231954582319546
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.0019739439399921043
 Normed ED: 0.0035881865857024567
 Normed ED: 0.002369668246445498
 Normed ED: 0.0024944567627494456
 Normed ED: 0.0030378348522507597
 Normed ED: 0.0038599393438103115
 Normed ED: 0.003872752420470263
 Normed ED: 0.06354883081155434
 Normed ED: 0.0031508467900748325
 Normed ED: 0.0023612750885478157
 Normed ED: 0.0023885350318471337
 Normed ED: 0.004150525733259546
 Normed ED: 0.0033259423503325942
 Normed ED: 0.002776675922253074
 Normed ED: 0.0043426766679826295
 Normed ED: 0.001664355062413315
 Normed ED: 0.0019770660340055358
 Normed ED: 0.0038642009384487995
 Normed ED: 0.00373366521468575
 Normed ED: 0.00234009360374415
 Normed ED: 0.003472222222222222
 Normed ED: 0.003738317757009346
 Normed ED: 0.00813953488372093
 Normed ED: 0.0025
 Normed ED: 0.01839080459770115
 Normed ED: 0.001877581674802854
 Normed ED: 0.04983748645720477
 Normed ED: 0.012697022767075307
 Normed ED: 0.0036334275333064193
 Normed ED: 0.4737550471063257
 Normed ED: 0.06140350877192982
 Normed ED: 0.010078878177037686
 Normed ED: 0.0028388928317956
 Normed ED: 0.5272158265340338
 Normed ED: 0.5725708502024291
 Normed ED: 0.3656135194709772
 Normed ED: 0.0016066838046272494
 Normed ED: 0.587062732069572
 Normed ED: 0.001903311762466692
 Normed ED: 0.0030807147258163892
 Normed ED: 0.13472992400903677
 Normed ED: 0.005696530295183843
 Normed ED: 0.0025758443045220377
 Normed ED: 0.0020460358056265983
 Normed ED: 0.05543187889581478
 Normed ED: 0.005076142131979695
 Normed ED: 0.23899493572263342
 Normed ED: 0.003167062549485352
 Normed ED: 0.4733622227886821
 Normed ED: 0.14564265817747712
 Normed ED: 0.37163221667854596
 Normed ED: 0.0032200357781753132
 Normed ED: 0.0034129692832764505
 Normed ED: 0.8216776087754353
 Normed ED: 0.625738616614529
 Normed ED: 0.835680047932894
 Normed ED: 0.8687236604455147
 Normed ED: 0.27938641847638745
 Normed ED: 0.10736196319018405
 Normed ED: 0.235480943738657
 Normed ED: 0.036601307189542485
 Normed ED: 0.03416557161629435
 Normed ED: 0.05684975422098739
 Normed ED: 0.24200671259494788
 Normed ED: 0.009725158562367865
 Normed ED: 0.06341933928140825
 Normed ED: 0.002008032128514056
 Normed ED: 0.0022868343678536427
 Normed ED: 0.11560693641618497
 Normed ED: 0.07010243277848911
 Normed ED: 0.8143707093821511
 Normed ED: 0.48349357118035446
 Normed ED: 0.4649791103465225
 Normed ED: 0.5161947661947662
 Normed ED: 0.2297558405058844
 Normed ED: 0.003219575016097875
 Normed ED: 0.003972983710766786
 Normed ED: 0.00267538644470868
 Normed ED: 0.0035650623885918
 Normed ED: 0.001201923076923077
 Normed ED: 0.7279050475400421
 Normed ED: 0.34723467862481316
 Normed ED: 0.7317330356035231
 Normed ED: 0.7881910529400273
 Normed ED: 0.7228489873589778
 Normed ED: 0.005293005671077505
 Normed ED: 0.0027808676307007787
 Normed ED: 0.0015180265654648956
 Normed ED: 0.4326723061988628
 Normed ED: 0.2735982966643009
 Normed ED: 0.2758070237672934
 Normed ED: 0.0017985611510791368
 Normed ED: 0.49818223131106565
 Normed ED: 0.09383215369059657
 Normed ED: 0.2674380165289256
 Normed ED: 0.0027149321266968325
 Normed ED: 0.04383282364933741
Pushing model to the hub, epoch 3
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/sebastian/anaconda3/envs/pt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(
 Normed ED: 0.3300376647834275
 Normed ED: 0.4657680028378858
 Normed ED: 0.0057361376673040155
 Normed ED: 0.11978221415607986
 Normed ED: 0.4773286976676807
 Normed ED: 0.07690487458589683
 Normed ED: 0.3059244126659857
 Normed ED: 0.33826994278020867
 Normed ED: 0.1225153595952295
 Normed ED: 0.06416131989000917
 Normed ED: 0.30133079847908745
 Normed ED: 0.2779973649538867
 Normed ED: 0.2157541217178913
 Normed ED: 0.17852684144818975
 Normed ED: 0.22814865794907088
 Normed ED: 0.004531722054380665
 Normed ED: 0.35055555555555556
 Normed ED: 0.00033266799733865603
 Normed ED: 0.004208311415044713
 Normed ED: 0.056436739210623386
 Normed ED: 0.3676423798918231
 Normed ED: 0.7962618700698387
 Normed ED: 0.4144823934339423
 Normed ED: 0.0005303632988597189
 Normed ED: 0.9112621359223301
 Normed ED: 0.002303389272787101
 Normed ED: 0.008563273073263558
 Normed ED: 0.009328358208955223
 Normed ED: 0.008466603951081843
 Normed ED: 0.008514664143803218
 Normed ED: 0.5167714884696016
 Normed ED: 0.7093732667775929
 Normed ED: 0.018518518518518517
 Normed ED: 0.07920792079207921
 Normed ED: 0.4342842905194967
 Normed ED: 0.2530186608122942
 Normed ED: 0.2218893197039126
 Normed ED: 0.002405002405002405
 Normed ED: 0.06408094435075885
 Normed ED: 0.041448842419716206
 Normed ED: 0.06292606187729417
 Normed ED: 0.4438133874239351
 Normed ED: 0.013496307613954673
 Normed ED: 0.6222690847743568
 Normed ED: 0.6366385484641095
 Normed ED: 0.006244218316373728
 Normed ED: 0.5727962085308057
 Normed ED: 0.28943035890681595
 Normed ED: 0.16054524801211661
 Normed ED: 0.5999261174732176
 Normed ED: 0.6490914738313402
 Normed ED: 0.0006071645415907711
 Normed ED: 0.28012718600953895
 Normed ED: 0.27139565503620805
 Normed ED: 0.5758728950623156
 Normed ED: 0.0027447392497712718
 Normed ED: 0.003676470588235294
 Normed ED: 0.0018198362147406734
 Normed ED: 0.002742230347349177
 Normed ED: 0.0018365472910927456
 Normed ED: 0.0018604651162790699
 Normed ED: 0.001834862385321101
 Normed ED: 0.0027347310847766638
 Normed ED: 0.0018248175182481751
 Normed ED: 0.0018535681186283596
 Normed ED: 0.0027548209366391185
 Normed ED: 0.0018331805682859762
 Normed ED: 0.004595588235294118
 Normed ED: 0.0018484288354898336
 Normed ED: 0.0018484288354898336
 Normed ED: 0.0027649769585253456
 Normed ED: 0.0027598896044158236
 Normed ED: 0.0027272727272727275
 Normed ED: 0.0018231540565177757
 Normed ED: 0.0018315018315018315
 Normed ED: 0.0018433179723502304
 Normed ED: 0.0027447392497712718
 Normed ED: 0.003639672429481347
 Normed ED: 0.0027548209366391185
 Normed ED: 0.0018433179723502304
 Normed ED: 0.0018501387604070306
 Normed ED: 0.003663003663003663
 Normed ED: 0.0018365472910927456
 Normed ED: 0.001834862385321101
 Normed ED: 0.003683241252302026
 Normed ED: 0.0018281535648994515
 Normed ED: 0.0018148820326678765
 Normed ED: 0.0018281535648994515
 Normed ED: 0.0018331805682859762
 Normed ED: 0.0027447392497712718
 Normed ED: 0.0018198362147406734
 Normed ED: 0.0018331805682859762
 Normed ED: 0.0018231540565177757
 Normed ED: 0.0036798528058877645
 Normed ED: 0.001671309192200557
 Normed ED: 0.0029190992493744786
 Normed ED: 0.0018812147272238647
 Normed ED: 0.002108433734939759
 Normed ED: 0.0021546564519990424
 Normed ED: 0.08108108108108109
 Normed ED: 0.0030120481927710845
 Normed ED: 0.0033222591362126247
 Normed ED: 0.0034602076124567475
 Normed ED: 0.19735562901030526
 Normed ED: 0.028228652081863093
 Normed ED: 0.2062754212667054
 Normed ED: 0.011352253756260434
 Normed ED: 0.004014720642355303
 Normed ED: 0.007177033492822967
 Normed ED: 0.009111617312072893
 Normed ED: 0.004550625711035267
 Normed ED: 0.008153026026967701
 Normed ED: 0.00787897888433659
 Normed ED: 0.020762551906379767
 Normed ED: 0.009766855702583491
 Normed ED: 0.0049410870391486126
 Normed ED: 0.004982871379632514
 Normed ED: 0.008679245283018867
 Normed ED: 0.011242973141786383
 Normed ED: 0.0040970690198550265
 Normed ED: 0.014420062695924765
 Normed ED: 0.004082914572864322
 Normed ED: 0.0067178502879078695
 Normed ED: 0.006605850896508336
 Normed ED: 0.009882174078297225
 Normed ED: 0.0031645569620253164
 Normed ED: 0.009984025559105431
 Normed ED: 0.6708820488348047
 Normed ED: 0.0046439628482972135
 Normed ED: 0.13317711384495215
 Normed ED: 0.8421634367401076
 Normed ED: 0.5320358626722065
 Normed ED: 0.0002752546105147261
 Normed ED: 0.0
 Normed ED: 0.20007183908045978
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.0004682744088035589
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.19838709677419356
 Normed ED: 0.000591016548463357
 Normed ED: 0.0
 Normed ED: 0.0005494505494505495
 Normed ED: 0.06697882736156352
 Normed ED: 0.0859517534968579
 Normed ED: 0.07548701298701299
 Normed ED: 0.000233590282644242
 Normed ED: 0.07380373073803731
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.0023677979479084454
 Normed ED: 0.002760143527463428
 Normed ED: 0.002369668246445498
 Normed ED: 0.0033250207813798837
 Normed ED: 0.0019331676332504833
 Normed ED: 0.004136789851075565
 Normed ED: 0.004427227448810182
 Normed ED: 0.0038503850385038503
 Normed ED: 0.0015754233950374162
 Normed ED: 0.0023612750885478157
 Normed ED: 0.002786624203821656
 Normed ED: 0.0035961272475795295
 Normed ED: 0.0033268644302744664
 Normed ED: 0.0019833399444664813
 Normed ED: 0.005921831819976312
 Normed ED: 0.0047130579428888274
 Normed ED: 0.0015810276679841897
 Normed ED: 0.005247169290251312
 Normed ED: 0.004978220286247666
 Normed ED: 0.0035101404056162248
 Normed ED: 0.002702702702702703
 Normed ED: 0.003738317757009346
 Normed ED: 0.004263565891472868
 Normed ED: 0.0025
 Normed ED: 0.3453846153846154
 Normed ED: 0.0022530980097634247
 Normed ED: 0.04875406283856988
 Normed ED: 0.0070052539404553416
 Normed ED: 0.003229713362939039
 Normed ED: 0.4499327052489906
 Normed ED: 0.011421628189550425
 Normed ED: 0.007011393514460999
 Normed ED: 0.0028388928317956
 Normed ED: 0.5266569799932939
 Normed ED: 0.5748987854251012
 Normed ED: 0.35547391623806024
 Normed ED: 0.00224791265253693
 Normed ED: 0.5859878835255032
 Normed ED: 0.001903311762466692
 Normed ED: 0.0030807147258163892
 Normed ED: 0.11521873074553296
 Normed ED: 0.04484304932735426
 Normed ED: 0.0017172295363480253
 Normed ED: 0.0020460358056265983
 Normed ED: 0.05276046304541407
 Normed ED: 0.0038071065989847717
 Normed ED: 0.27580833657966497
 Normed ED: 0.001583531274742676
 Normed ED: 0.46941116492480245
 Normed ED: 0.14146438519697574
 Normed ED: 0.3972915181753386
 Normed ED: 0.004287245444801715
 Normed ED: 0.0037921880925293893
 Normed ED: 0.82204461118134
 Normed ED: 0.625564824469934
 Normed ED: 0.835530257639305
 Normed ED: 0.8683323299217339
 Normed ED: 0.27249224405377453
 Normed ED: 0.12876344086021504
 Normed ED: 0.14632486388384755
 Normed ED: 0.03418803418803419
 Normed ED: 0.02774108322324967
 Normed ED: 0.05556742893780722
 Normed ED: 0.24077018194665253
 Normed ED: 0.00718816067653277
 Normed ED: 0.0027603513174404015
 Normed ED: 0.0017570281124497991
 Normed ED: 0.001961425302386401
 Normed ED: 0.004437273093989512
 Normed ED: 0.004802744425385935
 Normed ED: 0.8178947368421052
 Normed ED: 0.4811768794161937
 Normed ED: 0.46399606782993363
 Normed ED: 0.5152295152295152
 Normed ED: 0.2255401370103636
 Normed ED: 0.009664948453608248
 Normed ED: 0.0031783869686134287
 Normed ED: 0.032602423542989034
 Normed ED: 0.03257422888440473
 Normed ED: 0.001201923076923077
 Normed ED: 0.7265649926616042
 Normed ED: 0.34723467862481316
 Normed ED: 0.7314849274283588
 Normed ED: 0.7881910529400273
 Normed ED: 0.722101400027185
 Normed ED: 0.003026863412788498
 Normed ED: 0.002224694104560623
 Normed ED: 0.0015180265654648956
 Normed ED: 0.4357232006656497
 Normed ED: 0.2735982966643009
 Normed ED: 0.27882227740333454
 Normed ED: 0.0017985611510791368
 Normed ED: 0.49636446262213135
 Normed ED: 0.09504550050556117
 Normed ED: 0.2674380165289256
 Normed ED: 0.0006793478260869565
 Normed ED: 0.010692464358452138
Pushing model to the hub, epoch 4
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/sebastian/anaconda3/envs/pt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(
 Normed ED: 0.25252525252525254
 Normed ED: 0.3282051282051282
 Normed ED: 0.08317399617590822
 Normed ED: 0.15461847389558234
 Normed ED: 0.45632333767926986
 Normed ED: 0.16897506925207756
 Normed ED: 0.3999197753710389
 Normed ED: 0.26994278020868395
 Normed ED: 0.09748781402324709
 Normed ED: 0.05013673655423884
 Normed ED: 0.661787072243346
 Normed ED: 0.055994729907773384
 Normed ED: 0.24730307347852637
 Normed ED: 0.25418060200668896
 Normed ED: 0.2259119064005506
 Normed ED: 0.0037764350453172208
 Normed ED: 0.3497222222222222
 Normed ED: 0.00033266799733865603
 Normed ED: 0.004208311415044713
 Normed ED: 0.003501945525291829
 Normed ED: 0.3588927776010181
 Normed ED: 0.7961613827061247
 Normed ED: 0.41329097167063805
 Normed ED: 0.0005303632988597189
 Normed ED: 0.5740776699029126
 Normed ED: 0.0019743336623889436
 Normed ED: 0.008563273073263558
 Normed ED: 0.008403361344537815
 Normed ED: 0.010348071495766699
 Normed ED: 0.008514664143803218
 Normed ED: 0.514907989750757
 Normed ED: 0.7071547420965059
 Normed ED: 0.006246746486205101
 Normed ED: 0.07900585976965044
 Normed ED: 0.43079606328640835
 Normed ED: 0.2438712038053421
 Normed ED: 0.22312301727176595
 Normed ED: 0.002405002405002405
 Normed ED: 0.12759977515458124
 Normed ED: 0.0022404779686333084
 Normed ED: 0.0033407572383073497
 Normed ED: 0.4430020283975659
 Normed ED: 0.0700280112044818
 Normed ED: 0.6263180092787853
 Normed ED: 0.6382301448352697
 Normed ED: 0.007169287696577244
 Normed ED: 0.5703317535545024
 Normed ED: 0.2678630227197893
 Normed ED: 0.14842862552063613
 Normed ED: 0.5937384558551903
 Normed ED: 0.654371796862867
 Normed ED: 0.0030358227079538553
 Normed ED: 0.28139904610492844
 Normed ED: 0.25905200789993416
 Normed ED: 0.5714013890210256
 Normed ED: 0.004574565416285453
 Normed ED: 0.001838235294117647
 Normed ED: 0.00272975432211101
 Normed ED: 0.003656307129798903
 Normed ED: 0.0027522935779816515
 Normed ED: 0.0018604651162790699
 Normed ED: 0.002749770852428964
 Normed ED: 0.0027347310847766638
 Normed ED: 0.0018248175182481751
 Normed ED: 0.0018535681186283596
 Normed ED: 0.0018365472910927456
 Normed ED: 0.0018331805682859762
 Normed ED: 0.003676470588235294
 Normed ED: 0.0018484288354898336
 Normed ED: 0.0018484288354898336
 Normed ED: 0.0018433179723502304
 Normed ED: 0.0027598896044158236
 Normed ED: 0.0027272727272727275
 Normed ED: 0.0018231540565177757
 Normed ED: 0.0018315018315018315
 Normed ED: 0.0018433179723502304
 Normed ED: 0.0027447392497712718
 Normed ED: 0.00272975432211101
 Normed ED: 0.0018365472910927456
 Normed ED: 0.0018433179723502304
 Normed ED: 0.0018501387604070306
 Normed ED: 0.0027472527472527475
 Normed ED: 0.0027522935779816515
 Normed ED: 0.001834862385321101
 Normed ED: 0.003683241252302026
 Normed ED: 0.0018281535648994515
 Normed ED: 0.0018148820326678765
 Normed ED: 0.0018281535648994515
 Normed ED: 0.0018331805682859762
 Normed ED: 0.0027447392497712718
 Normed ED: 0.0018198362147406734
 Normed ED: 0.002749770852428964
 Normed ED: 0.0018231540565177757
 Normed ED: 0.004595588235294118
 Normed ED: 0.002229654403567447
 Normed ED: 0.0029190992493744786
 Normed ED: 0.0018812147272238647
 Normed ED: 0.002108433734939759
 Normed ED: 0.0016766467065868263
 Normed ED: 0.08324324324324324
 Normed ED: 0.0024103645676408557
 Normed ED: 0.0033222591362126247
 Normed ED: 0.002129358530742614
 Normed ED: 0.19463348240326658
 Normed ED: 0.028228652081863093
 Normed ED: 0.20685647879140034
 Normed ED: 0.0192243950944647
 Normed ED: 0.0016722408026755853
 Normed ED: 0.006220095693779904
 Normed ED: 0.0056947608200455585
 Normed ED: 0.007196969696969697
 Normed ED: 0.0037652965171007216
 Normed ED: 0.006618342262842736
 Normed ED: 0.009437523593808984
 Normed ED: 0.009136735979836169
 Normed ED: 0.0034207525655644243
 Normed ED: 0.0062266500622665
 Normed ED: 0.00830188679245283
 Normed ED: 0.0028089887640449437
 Normed ED: 0.001890359168241966
 Normed ED: 0.0065830721003134795
 Normed ED: 0.005339195979899497
 Normed ED: 0.004322766570605188
 Normed ED: 0.00847723704866562
 Normed ED: 0.0049410870391486126
 Normed ED: 0.006329113924050633
 Normed ED: 0.006389776357827476
 Normed ED: 0.6701662292213474
 Normed ED: 0.0046439628482972135
 Normed ED: 0.13317711384495215
 Normed ED: 0.8416476309778204
 Normed ED: 0.5312705007653619
 Normed ED: 0.000275178866263071
 Normed ED: 0.0
 Normed ED: 0.1939655172413793
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.00023413720440177945
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.1992831541218638
 Normed ED: 0.000591016548463357
 Normed ED: 0.0
 Normed ED: 0.0008241758241758242
 Normed ED: 0.06779315960912052
 Normed ED: 0.08676261909588485
 Normed ED: 0.07528409090909091
 Normed ED: 0.0
 Normed ED: 0.07583130575831305
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.0039494470774091624
 Normed ED: 0.0041390728476821195
 Normed ED: 0.002369668246445498
 Normed ED: 0.003048780487804878
 Normed ED: 0.0016565433462175593
 Normed ED: 0.0041356492969396195
 Normed ED: 0.004149377593360996
 Normed ED: 0.005496015388843089
 Normed ED: 0.002362204724409449
 Normed ED: 0.0027526543452615023
 Normed ED: 0.0027844073190135244
 Normed ED: 0.005535566011624689
 Normed ED: 0.0027723870252287217
 Normed ED: 0.0019833399444664813
 Normed ED: 0.007106198183971575
 Normed ED: 0.0024958402662229617
 Normed ED: 0.0031633056544088573
 Normed ED: 0.0038663352665009665
 Normed ED: 0.00373366521468575
 Normed ED: 0.004290171606864275
 Normed ED: 0.002317497103128621
 Normed ED: 0.0043586550435865505
 Normed ED: 0.006201550387596899
 Normed ED: 0.0025
 Normed ED: 0.008461538461538461
 Normed ED: 0.0030041306796845663
 Normed ED: 0.04897074756229686
 Normed ED: 0.009632224168126095
 Normed ED: 0.003228410008071025
 Normed ED: 0.45006729475100943
 Normed ED: 0.007290400972053463
 Normed ED: 0.0026292725679228747
 Normed ED: 0.00248403122782115
 Normed ED: 0.5332513691740248
 Normed ED: 0.5734817813765182
 Normed ED: 0.35797207935341663
 Normed ED: 0.0019280205655526992
 Normed ED: 0.5872581590775845
 Normed ED: 0.001903311762466692
 Normed ED: 0.004313000616142945
 Normed ED: 0.13123844731977818
 Normed ED: 0.005175983436853002
 Normed ED: 0.0025758443045220377
 Normed ED: 0.0033239580669905395
 Normed ED: 0.05365093499554764
 Normed ED: 0.005076142131979695
 Normed ED: 0.22574990261005065
 Normed ED: 0.0011876484560570072
 Normed ED: 0.4713229671170023
 Normed ED: 0.13808197373656983
 Normed ED: 0.37077690662865287
 Normed ED: 0.0032177332856632105
 Normed ED: 0.0007590132827324478
 Normed ED: 0.8213106063695307
 Normed ED: 0.6225234619395204
 Normed ED: 0.8337702216896345
 Normed ED: 0.8679108970499699
 Normed ED: 0.27542226818338506
 Normed ED: 0.11226993865030675
 Normed ED: 0.1413339382940109
 Normed ED: 0.08031599736668861
 Normed ED: 0.04095112285336856
 Normed ED: 0.05599487069886728
 Normed ED: 0.24024024024024024
 Normed ED: 0.0038054968287526427
 Normed ED: 0.0017565872020075283
 Normed ED: 0.11741586833701793
 Normed ED: 0.00196078431372549
 Normed ED: 0.0036290322580645163
 Normed ED: 0.003089598352214212
 Normed ED: 0.8137299771167048
 Normed ED: 0.4836094057685625
 Normed ED: 0.4653477512902433
 Normed ED: 0.5153367653367653
 Normed ED: 0.230985420692078
 Normed ED: 0.007087628865979381
 Normed ED: 0.0031783869686134287
 Normed ED: 0.00267538644470868
 Normed ED: 0.0053475935828877
 Normed ED: 0.001201923076923077
 Normed ED: 0.726947865484015
 Normed ED: 0.34723467862481316
 Normed ED: 0.7314849274283588
 Normed ED: 0.7881910529400273
 Normed ED: 0.7225771374201441
 Normed ED: 0.0026485054861899358
 Normed ED: 0.002224694104560623
 Normed ED: 0.0015180265654648956
 Normed ED: 0.43461378449590904
 Normed ED: 0.2735982966643009
 Normed ED: 0.27882227740333454
 Normed ED: 0.002696629213483146
 Normed ED: 0.49761417859577367
 Normed ED: 0.0942366026289181
 Normed ED: 0.2674380165289256
 Normed ED: 0.0009057971014492754
 Normed ED: 0.0061162079510703364
Pushing model to the hub, epoch 5
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/sebastian/anaconda3/envs/pt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(
 Normed ED: 0.23759791122715404
 Normed ED: 0.3082653423199716
 Normed ED: 0.0057361376673040155
 Normed ED: 0.006147540983606557
 Normed ED: 0.4613935969868173
 Normed ED: 0.07415304430229804
 Normed ED: 0.2977346278317152
 Normed ED: 0.280713564456412
 Normed ED: 0.08903278025091056
 Normed ED: 0.23978685612788633
 Normed ED: 0.32547528517110264
 Normed ED: 0.037549407114624504
 Normed ED: 0.16242621616120498
 Normed ED: 0.061052631578947365
 Normed ED: 0.2259119064005506
 Normed ED: 0.0037764350453172208
 Normed ED: 0.3526388888888889
 Normed ED: 0.00033266799733865603
 Normed ED: 0.003682272488164124
 Normed ED: 0.0038910505836575876
 Normed ED: 0.36223353483932547
 Normed ED: 0.798171129980405
 Normed ED: 0.41501191421763306
 Normed ED: 0.0005303632988597189
 Normed ED: 0.574368932038835
 Normed ED: 0.0019743336623889436
 Normed ED: 0.008563273073263558
 Normed ED: 0.008403361344537815
 Normed ED: 0.008466603951081843
 Normed ED: 0.008514664143803218
 Normed ED: 0.5262054507337526
 Normed ED: 0.7108985024958403
 Normed ED: 0.002082248828735034
 Normed ED: 0.07839967670236411
 Normed ED: 0.4346580291516133
 Normed ED: 0.2504573728503476
 Normed ED: 0.2224180472329926
 Normed ED: 0.001924001924001924
 Normed ED: 0.0016863406408094434
 Normed ED: 0.0022404779686333084
 Normed ED: 0.0033407572383073497
 Normed ED: 0.4438133874239351
 Normed ED: 0.006097560975609756
 Normed ED: 0.6257275411218896
 Normed ED: 0.6368772879197836
 Normed ED: 0.006244218316373728
 Normed ED: 0.5711848341232227
 Normed ED: 0.2716496542640764
 Normed ED: 0.14615675880348353
 Normed ED: 0.592445511636498
 Normed ED: 0.6545271004814412
 Normed ED: 0.0
 Normed ED: 0.2809220985691574
 Normed ED: 0.26925608953258723
 Normed ED: 0.5733992959756445
 Normed ED: 0.0027447392497712718
 Normed ED: 0.001838235294117647
 Normed ED: 0.0018198362147406734
 Normed ED: 0.0018281535648994515
 Normed ED: 0.0027522935779816515
 Normed ED: 0.0037209302325581397
 Normed ED: 0.001834862385321101
 Normed ED: 0.0027347310847766638
 Normed ED: 0.0018248175182481751
 Normed ED: 0.0018535681186283596
 Normed ED: 0.0027548209366391185
 Normed ED: 0.0018331805682859762
 Normed ED: 0.003676470588235294
 Normed ED: 0.0018484288354898336
 Normed ED: 0.0018484288354898336
 Normed ED: 0.0018433179723502304
 Normed ED: 0.001841620626151013
 Normed ED: 0.0018181818181818182
 Normed ED: 0.0018231540565177757
 Normed ED: 0.0018315018315018315
 Normed ED: 0.0018433179723502304
 Normed ED: 0.003663003663003663
 Normed ED: 0.00272975432211101
 Normed ED: 0.0027548209366391185
 Normed ED: 0.004608294930875576
 Normed ED: 0.0018501387604070306
 Normed ED: 0.0027447392497712718
 Normed ED: 0.0027522935779816515
 Normed ED: 0.001834862385321101
 Normed ED: 0.006445672191528545
 Normed ED: 0.0018281535648994515
 Normed ED: 0.0027223230490018148
 Normed ED: 0.0018281535648994515
 Normed ED: 0.0018331805682859762
 Normed ED: 0.0018315018315018315
 Normed ED: 0.0018198362147406734
 Normed ED: 0.0027472527472527475
 Normed ED: 0.0018231540565177757
 Normed ED: 0.0036798528058877645
 Normed ED: 0.0011148272017837235
 Normed ED: 0.0029190992493744786
 Normed ED: 0.002418704649287826
 Normed ED: 0.002108433734939759
 Normed ED: 0.0016766467065868263
 Normed ED: 0.08475675675675676
 Normed ED: 0.0036155468514612837
 Normed ED: 0.0029069767441860465
 Normed ED: 0.0018631887143997872
 Normed ED: 0.19735562901030526
 Normed ED: 0.02869912961656081
 Normed ED: 0.20491962037575054
 Normed ED: 0.012020033388981636
 Normed ED: 0.0016728002676480427
 Normed ED: 0.004784688995215311
 Normed ED: 0.007593014426727411
 Normed ED: 0.005686125852918878
 Normed ED: 0.004706620646375902
 Normed ED: 0.004727387330601954
 Normed ED: 0.008295625942684766
 Normed ED: 0.00945179584120983
 Normed ED: 0.0049410870391486126
 Normed ED: 0.003737153534724385
 Normed ED: 0.005281026027913994
 Normed ED: 0.0049921996879875195
 Normed ED: 0.001575795776867318
 Normed ED: 0.004702194357366771
 Normed ED: 0.004080351537978657
 Normed ED: 0.005283381364073006
 Normed ED: 0.006595477386934673
 Normed ED: 0.005321170657544659
 Normed ED: 0.004922644163150493
 Normed ED: 0.035339805825242716
 Normed ED: 0.6708820488348047
 Normed ED: 0.005417956656346749
 Normed ED: 0.1335676625659051
 Normed ED: 0.842273966546312
 Normed ED: 0.5318171878416794
 Normed ED: 0.0002752546105147261
 Normed ED: 0.0
 Normed ED: 0.1966594827586207
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.00023413720440177945
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.19874551971326165
 Normed ED: 0.000591016548463357
 Normed ED: 0.0
 Normed ED: 0.0005494505494505495
 Normed ED: 0.06677524429967427
 Normed ED: 0.08554632069734441
 Normed ED: 0.07528409090909091
 Normed ED: 0.0
 Normed ED: 0.07887266828872669
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.002764612954186414
 Normed ED: 0.004140215291195142
 Normed ED: 0.0035545023696682463
 Normed ED: 0.0024944567627494456
 Normed ED: 0.0022093344380005524
 Normed ED: 0.003031973539140022
 Normed ED: 0.00331858407079646
 Normed ED: 0.03328748280605227
 Normed ED: 0.0023631350925561244
 Normed ED: 0.003147128245476003
 Normed ED: 0.0031821797931583136
 Normed ED: 0.004703929164360819
 Normed ED: 0.0041574279379157425
 Normed ED: 0.0015860428231562252
 Normed ED: 0.008685353335965259
 Normed ED: 0.0024965325936199723
 Normed ED: 0.002372479240806643
 Normed ED: 0.0035881865857024567
 Normed ED: 0.00373366521468575
 Normed ED: 0.005070202808112325
 Normed ED: 0.002317497103128621
 Normed ED: 0.003738317757009346
 Normed ED: 0.006589147286821705
 Normed ED: 0.0025
 Normed ED: 0.007692307692307693
 Normed ED: 0.0015020653398422831
 Normed ED: 0.04897074756229686
 Normed ED: 0.005691768826619965
 Normed ED: 0.002825999192571659
 Normed ED: 0.44724091520861375
 Normed ED: 0.008748481166464156
 Normed ED: 0.004820333041191937
 Normed ED: 0.0021291696238466998
 Normed ED: 0.5203978987370068
 Normed ED: 0.5688259109311741
 Normed ED: 0.35635562086700956
 Normed ED: 0.0016066838046272494
 Normed ED: 0.586281024037522
 Normed ED: 0.001903311762466692
 Normed ED: 0.0030807147258163892
 Normed ED: 0.11562949270897514
 Normed ED: 0.004660797514241326
 Normed ED: 0.0034344590726960505
 Normed ED: 0.002812579902838149
 Normed ED: 0.05253784505788068
 Normed ED: 0.005076142131979695
 Normed ED: 0.22536034281262174
 Normed ED: 0.0023752969121140144
 Normed ED: 0.4699209788427224
 Normed ED: 0.1376840429765221
 Normed ED: 0.37248752672843904
 Normed ED: 0.0025044722719141325
 Normed ED: 0.0011385199240986717
 Normed ED: 0.8217183868205358
 Normed ED: 0.625564824469934
 Normed ED: 0.8357923906530856
 Normed ED: 0.8686935580975316
 Normed ED: 0.2687004481213375
 Normed ED: 0.03822222222222222
 Normed ED: 0.13747731397459165
 Normed ED: 0.031475409836065574
 Normed ED: 0.14022578728461083
 Normed ED: 0.04979696516349647
 Normed ED: 0.24006359300476948
 Normed ED: 0.0071790540540540545
 Normed ED: 0.002007024586051179
 Normed ED: 0.0015060240963855422
 Normed ED: 0.0013076168682576005
 Normed ED: 0.003630496167809601
 Normed ED: 0.002403020940611054
 Normed ED: 0.8135469107551487
 Normed ED: 0.4810610448279856
 Normed ED: 0.4625215040550504
 Normed ED: 0.5173745173745173
 Normed ED: 0.2260670999473037
 Normed ED: 0.003219575016097875
 Normed ED: 0.003575685339690107
 Normed ED: 0.0035671819262782403
 Normed ED: 0.00267379679144385
 Normed ED: 0.001201923076923077
 Normed ED: 0.7265649926616042
 Normed ED: 0.34723467862481316
 Normed ED: 0.7314229003845677
 Normed ED: 0.7881910529400273
 Normed ED: 0.7341307598205791
 Normed ED: 0.00340522133938706
 Normed ED: 0.002224694104560623
 Normed ED: 0.0015180265654648956
 Normed ED: 0.43600055470808485
 Normed ED: 0.2735982966643009
 Normed ED: 0.27829017382050375
 Normed ED: 0.0013489208633093526
 Normed ED: 0.4973869575096569
 Normed ED: 0.09241658240647119
 Normed ED: 0.2674380165289256
 Normed ED: 0.0009057971014492754
 Normed ED: 0.0056065239551478085
Pushing model to the hub, epoch 6
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/sebastian/anaconda3/envs/pt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 Normed ED: 0.06471631205673758
 Normed ED: 0.9602695991486343
 Normed ED: 0.08508604206500955
 Normed ED: 0.14209591474245115
 Normed ED: 0.4631319716065479
 Normed ED: 0.09881796690307329
 Normed ED: 0.21751101321585903
 Normed ED: 0.2647256815886907
 Normed ED: 0.11698841698841698
 Normed ED: 0.1965648854961832
 Normed ED: 0.3015209125475285
 Normed ED: 0.1297760210803689
 Normed ED: 0.1929574598005292
 Normed ED: 0.06938483547925609
 Normed ED: 0.2259119064005506
 Normed ED: 0.004531722054380665
 Normed ED: 0.3611111111111111
 Normed ED: 0.003654485049833887
 Normed ED: 0.003682272488164124
 Normed ED: 0.0038910505836575876
 Normed ED: 0.40200445434298443
 Normed ED: 0.7961111390242677
 Normed ED: 0.4126290706910246
 Normed ED: 0.0007953340402969247
 Normed ED: 0.5737864077669903
 Normed ED: 0.002303389272787101
 Normed ED: 0.008563273073263558
 Normed ED: 0.008403361344537815
 Normed ED: 0.008466603951081843
 Normed ED: 0.00945179584120983
 Normed ED: 0.5112974609829956
 Normed ED: 0.7084026622296173
 Normed ED: 0.006243496357960458
 Normed ED: 0.07779349363507779
 Normed ED: 0.43216643827083595
 Normed ED: 0.24094401756311745
 Normed ED: 0.2218893197039126
 Normed ED: 0.001924001924001924
 Normed ED: 0.06408094435075885
 Normed ED: 0.005601194921583271
 Normed ED: 0.0033407572383073497
 Normed ED: 0.44164976335361733
 Normed ED: 0.004578987534978377
 Normed ED: 0.6231969633066217
 Normed ED: 0.6383097246538277
 Normed ED: 0.00786308973172988
 Normed ED: 0.5688151658767773
 Normed ED: 0.26489957194599933
 Normed ED: 0.13404013631200304
 Normed ED: 0.5910602142593276
 Normed ED: 0.6517316353471035
 Normed ED: 0.0018214936247723133
 Normed ED: 0.2777424483306836
 Normed ED: 0.26547070441079657
 Normed ED: 0.5732090191228237
 Normed ED: 0.0027447392497712718
 Normed ED: 0.001838235294117647
 Normed ED: 0.00272975432211101
 Normed ED: 0.002742230347349177
 Normed ED: 0.0018365472910927456
 Normed ED: 0.0018604651162790699
 Normed ED: 0.001834862385321101
 Normed ED: 0.0018231540565177757
 Normed ED: 0.0018248175182481751
 Normed ED: 0.0018535681186283596
 Normed ED: 0.0018365472910927456
 Normed ED: 0.0018331805682859762
 Normed ED: 0.003676470588235294
 Normed ED: 0.0018484288354898336
 Normed ED: 0.0018484288354898336
 Normed ED: 0.0027649769585253456
 Normed ED: 0.001841620626151013
 Normed ED: 0.0018181818181818182
 Normed ED: 0.0018231540565177757
 Normed ED: 0.0018315018315018315
 Normed ED: 0.0018433179723502304
 Normed ED: 0.003663003663003663
 Normed ED: 0.0770625566636446
 Normed ED: 0.0018365472910927456
 Normed ED: 0.0027649769585253456
 Normed ED: 0.0027752081406105457
 Normed ED: 0.0018315018315018315
 Normed ED: 0.0018365472910927456
 Normed ED: 0.001834862385321101
 Normed ED: 0.003683241252302026
 Normed ED: 0.002742230347349177
 Normed ED: 0.0018148820326678765
 Normed ED: 0.0018281535648994515
 Normed ED: 0.0018331805682859762
 Normed ED: 0.0018315018315018315
 Normed ED: 0.00272975432211101
 Normed ED: 0.0018331805682859762
 Normed ED: 0.0027347310847766638
 Normed ED: 0.006445672191528545
 Normed ED: 0.002508361204013378
 Normed ED: 0.003336113427856547
 Normed ED: 0.0018812147272238647
 Normed ED: 0.002108433734939759
 Normed ED: 0.0016766467065868263
 Normed ED: 0.08216216216216216
 Normed ED: 0.0021090689966857487
 Normed ED: 0.0033222591362126247
 Normed ED: 0.002129358530742614
 Normed ED: 0.19735562901030526
 Normed ED: 0.027993413314514232
 Normed ED: 0.2053069920588805
 Normed ED: 0.020958083832335328
 Normed ED: 0.003011040481766477
 Normed ED: 0.0028694404591104736
 Normed ED: 0.004554079696394687
 Normed ED: 0.005303030303030303
 Normed ED: 0.005018820577164366
 Normed ED: 0.006933501418216199
 Normed ED: 0.009437523593808984
 Normed ED: 0.007876496534341524
 Normed ED: 0.006461421512732801
 Normed ED: 0.009342883836810962
 Normed ED: 0.007541478129713424
 Normed ED: 0.004370902279113331
 Normed ED: 0.00441222817522849
 Normed ED: 0.007836990595611285
 Normed ED: 0.004085480829666876
 Normed ED: 0.004803073967339097
 Normed ED: 0.006599622878692646
 Normed ED: 0.006461421512732801
 Normed ED: 0.005625879043600563
 Normed ED: 0.0059904153354632585
 Normed ED: 0.6703253002465601
 Normed ED: 0.09597523219814241
 Normed ED: 0.13317711384495215
 Normed ED: 0.8452214280450961
 Normed ED: 0.531926525256943
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.19576149425287356
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.00023413720440177945
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.19874551971326165
 Normed ED: 0.000591016548463357
 Normed ED: 0.000591715976331361
 Normed ED: 0.0019230769230769232
 Normed ED: 0.06677524429967427
 Normed ED: 0.08554632069734441
 Normed ED: 0.07386363636363637
 Normed ED: 0.0
 Normed ED: 0.07887266828872669
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.001579778830963665
 Normed ED: 0.0038642009384487995
 Normed ED: 0.002369668246445498
 Normed ED: 0.002770850651149903
 Normed ED: 0.0024855012427506215
 Normed ED: 0.003861003861003861
 Normed ED: 0.0035971223021582736
 Normed ED: 0.004672897196261682
 Normed ED: 0.0027559055118110236
 Normed ED: 0.0023603461841070024
 Normed ED: 0.0027844073190135244
 Normed ED: 0.0027677830058123443
 Normed ED: 0.0027723870252287217
 Normed ED: 0.001190003966679889
 Normed ED: 0.002368732727990525
 Normed ED: 0.0024965325936199723
 Normed ED: 0.002372479240806643
 Normed ED: 0.0035901684617508974
 Normed ED: 0.005600497822028625
 Normed ED: 0.00234009360374415
 Normed ED: 0.002317497103128621
 Normed ED: 0.003738317757009346
 Normed ED: 0.006194347657762292
 Normed ED: 0.0025
 Normed ED: 0.006153846153846154
 Normed ED: 0.001877581674802854
 Normed ED: 0.04875406283856988
 Normed ED: 0.1409142425387231
 Normed ED: 0.025030278562777553
 Normed ED: 0.4473755047106326
 Normed ED: 0.007533414337788579
 Normed ED: 0.0052585451358457495
 Normed ED: 0.0017743080198722497
 Normed ED: 0.5233039007488544
 Normed ED: 0.5692307692307692
 Normed ED: 0.35547391623806024
 Normed ED: 0.0016066838046272494
 Normed ED: 0.5859878835255032
 Normed ED: 0.001903311762466692
 Normed ED: 0.0030807147258163892
 Normed ED: 0.11028958718422674
 Normed ED: 0.004658385093167702
 Normed ED: 0.0028620492272467086
 Normed ED: 0.0020460358056265983
 Normed ED: 0.05120213713268032
 Normed ED: 0.0076045627376425855
 Normed ED: 0.22185430463576158
 Normed ED: 0.0011876484560570072
 Normed ED: 0.46660718837624265
 Normed ED: 0.13967369677676084
 Normed ED: 0.3697790449037776
 Normed ED: 0.0025044722719141325
 Normed ED: 0.0018975332068311196
 Normed ED: 0.8222077233617421
 Normed ED: 0.625564824469934
 Normed ED: 0.8361668663870582
 Normed ED: 0.8686935580975316
 Normed ED: 0.26852809376077214
 Normed ED: 0.004601226993865031
 Normed ED: 0.14609800362976408
 Normed ED: 0.04409857328145266
 Normed ED: 0.04415584415584416
 Normed ED: 0.049369523402436416
 Normed ED: 0.2319378201731143
 Normed ED: 0.0063371356147021544
 Normed ED: 0.0017565872020075283
 Normed ED: 0.0015060240963855422
 Normed ED: 0.0013076168682576005
 Normed ED: 0.003227107704719645
 Normed ED: 0.0034328870580157913
 Normed ED: 0.8129061784897025
 Normed ED: 0.4803660372987374
 Normed ED: 0.46399606782993363
 Normed ED: 0.5141570141570142
 Normed ED: 0.22993149481819777
 Normed ED: 0.003219575016097875
 Normed ED: 0.0027810885975367503
 Normed ED: 0.06391200951248514
 Normed ED: 0.033273915626856804
 Normed ED: 0.001201923076923077
 Normed ED: 0.7265649926616042
 Normed ED: 0.34723467862481316
 Normed ED: 0.7314229003845677
 Normed ED: 0.7882398906036335
 Normed ED: 0.7201304879706402
 Normed ED: 0.0026485054861899358
 Normed ED: 0.002224694104560623
 Normed ED: 0.0015180265654648956
 Normed ED: 0.4323949521564277
 Normed ED: 0.2735982966643009
 Normed ED: 0.27829017382050375
 Normed ED: 0.0022471910112359553
 Normed ED: 0.4970461258804817
 Normed ED: 0.0968655207280081
 Normed ED: 0.2674380165289256
 Normed ED: 0.0006793478260869565
 Normed ED: 0.006625891946992864
Pushing model to the hub, epoch 7
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/sebastian/anaconda3/envs/pt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 Normed ED: 0.09936034115138592
 Normed ED: 0.38275984391628237
 Normed ED: 0.0124282982791587
 Normed ED: 0.00205761316872428
 Normed ED: 0.4361871650007243
 Normed ED: 0.11691312384473197
 Normed ED: 0.4629333333333333
 Normed ED: 0.2950185122854258
 Normed ED: 0.08336705787130716
 Normed ED: 0.2467924528301887
 Normed ED: 0.2661596958174905
 Normed ED: 0.08739708676377454
 Normed ED: 0.1892937105638103
 Normed ED: 0.04918032786885246
 Normed ED: 0.229525120440468
 Normed ED: 0.0037764350453172208
 Normed ED: 0.37916666666666665
 Normed ED: 0.02162341982701264
 Normed ED: 0.06417674907943188
 Normed ED: 0.002723735408560311
 Normed ED: 0.35491568565065224
 Normed ED: 0.7960106516605537
 Normed ED: 0.41329097167063805
 Normed ED: 0.0005303632988597189
 Normed ED: 0.5742718446601942
 Normed ED: 0.0019743336623889436
 Normed ED: 0.008563273073263558
 Normed ED: 0.008403361344537815
 Normed ED: 0.008466603951081843
 Normed ED: 0.00945179584120983
 Normed ED: 0.5122292103424179
 Normed ED: 0.7084719911259013
 Normed ED: 0.009885535900104058
 Normed ED: 0.07981410385936553
 Normed ED: 0.43092064283044723
 Normed ED: 0.24222466154409075
 Normed ED: 0.2218893197039126
 Normed ED: 0.001924001924001924
 Normed ED: 0.002529510961214165
 Normed ED: 0.0033607169529499626
 Normed ED: 0.0033407572383073497
 Normed ED: 0.4407031778228533
 Normed ED: 0.0073847720906544435
 Normed ED: 0.6233656684943062
 Normed ED: 0.6370364475568996
 Normed ED: 0.004162812210915819
 Normed ED: 0.566824644549763
 Normed ED: 0.2667105696410932
 Normed ED: 0.13706929193487316
 Normed ED: 0.592168452161064
 Normed ED: 0.6498679919242119
 Normed ED: 0.0024286581663630845
 Normed ED: 0.2812400635930048
 Normed ED: 0.2564186965108624
 Normed ED: 0.5744458186661593
 Normed ED: 0.0027447392497712718
 Normed ED: 0.001838235294117647
 Normed ED: 0.0018198362147406734
 Normed ED: 0.0027397260273972603
 Normed ED: 0.0018365472910927456
 Normed ED: 0.0018604651162790699
 Normed ED: 0.001834862385321101
 Normed ED: 0.0018231540565177757
 Normed ED: 0.0027347310847766638
 Normed ED: 0.0018535681186283596
 Normed ED: 0.0027548209366391185
 Normed ED: 0.0018331805682859762
 Normed ED: 0.004595588235294118
 Normed ED: 0.0018484288354898336
 Normed ED: 0.0046210720887245845
 Normed ED: 0.0018433179723502304
 Normed ED: 0.001841620626151013
 Normed ED: 0.0027272727272727275
 Normed ED: 0.0018231540565177757
 Normed ED: 0.0018315018315018315
 Normed ED: 0.0027624309392265192
 Normed ED: 0.003663003663003663
 Normed ED: 0.003639672429481347
 Normed ED: 0.0018365472910927456
 Normed ED: 0.0027624309392265192
 Normed ED: 0.0018501387604070306
 Normed ED: 0.0045662100456621
 Normed ED: 0.0027548209366391185
 Normed ED: 0.001834862385321101
 Normed ED: 0.003676470588235294
 Normed ED: 0.0018281535648994515
 Normed ED: 0.0018148820326678765
 Normed ED: 0.0018281535648994515
 Normed ED: 0.0018331805682859762
 Normed ED: 0.0018315018315018315
 Normed ED: 0.003639672429481347
 Normed ED: 0.002749770852428964
 Normed ED: 0.0027347310847766638
 Normed ED: 0.003683241252302026
 Normed ED: 0.0011148272017837235
 Normed ED: 0.0029190992493744786
 Normed ED: 0.0018812147272238647
 Normed ED: 0.002108433734939759
 Normed ED: 0.0026334689968877186
 Normed ED: 0.08302702702702702
 Normed ED: 0.0021090689966857487
 Normed ED: 0.0029069767441860465
 Normed ED: 0.002129358530742614
 Normed ED: 0.19755006805366518
 Normed ED: 0.027993413314514232
 Normed ED: 0.20491962037575054
 Normed ED: 0.04574290484140234
 Normed ED: 0.0016728002676480427
 Normed ED: 0.00861244019138756
 Normed ED: 0.00683371298405467
 Normed ED: 0.005309063329541145
 Normed ED: 0.0025101976780671476
 Normed ED: 0.006618342262842736
 Normed ED: 0.007927519818799546
 Normed ED: 0.006616257088846881
 Normed ED: 0.004180919802356519
 Normed ED: 0.004671441918405481
 Normed ED: 0.004528301886792453
 Normed ED: 0.0034321372854914196
 Normed ED: 0.003778337531486146
 Normed ED: 0.008463949843260187
 Normed ED: 0.007228158390949088
 Normed ED: 0.00672107537205953
 Normed ED: 0.007223618090452261
 Normed ED: 0.005321170657544659
 Normed ED: 0.0035161744022503515
 Normed ED: 0.003194888178913738
 Normed ED: 0.6696890161457091
 Normed ED: 0.0046439628482972135
 Normed ED: 0.13337238820542863
 Normed ED: 0.8406897059907155
 Normed ED: 0.5640717253444129
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.19360632183908047
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.00023413720440177945
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.19874551971326165
 Normed ED: 0.017139479905437353
 Normed ED: 0.000591715976331361
 Normed ED: 0.0002747252747252747
 Normed ED: 0.06677524429967427
 Normed ED: 0.0869653354956416
 Normed ED: 0.07528409090909091
 Normed ED: 0.0
 Normed ED: 0.07887266828872669
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.00315955766192733
 Normed ED: 0.0038642009384487995
 Normed ED: 0.002764612954186414
 Normed ED: 0.002771618625277162
 Normed ED: 0.0024855012427506215
 Normed ED: 0.0016542597187758478
 Normed ED: 0.003872752420470263
 Normed ED: 0.004125412541254125
 Normed ED: 0.0031508467900748325
 Normed ED: 0.0019677292404565133
 Normed ED: 0.0027844073190135244
 Normed ED: 0.0038748962081372822
 Normed ED: 0.00249514832270585
 Normed ED: 0.0019833399444664813
 Normed ED: 0.006711409395973154
 Normed ED: 0.0030513176144244107
 Normed ED: 0.002766798418972332
 Normed ED: 0.004142502071251036
 Normed ED: 0.004975124378109453
 Normed ED: 0.00234009360374415
 Normed ED: 0.002317497103128621
 Normed ED: 0.003738317757009346
 Normed ED: 0.0031007751937984496
 Normed ED: 0.0025
 Normed ED: 0.006153846153846154
 Normed ED: 0.0011265490048817123
 Normed ED: 0.04918743228602383
 Normed ED: 0.006129597197898424
 Normed ED: 0.004037141703673799
 Normed ED: 0.45020188425302826
 Normed ED: 0.00558659217877095
 Normed ED: 0.0030661410424879547
 Normed ED: 0.0014194464158978
 Normed ED: 0.5229685928244104
 Normed ED: 0.5695344129554656
 Normed ED: 0.35547391623806024
 Normed ED: 0.0016066838046272494
 Normed ED: 0.5859878835255032
 Normed ED: 0.001903311762466692
 Normed ED: 0.0036968576709796672
 Normed ED: 0.1143972068186486
 Normed ED: 0.005175983436853002
 Normed ED: 0.0022896393817973667
 Normed ED: 0.037595907928388746
 Normed ED: 0.15672306322350846
 Normed ED: 0.0038071065989847717
 Normed ED: 0.22672380210362292
 Normed ED: 0.001979414093428345
 Normed ED: 0.4664797348967627
 Normed ED: 0.2075208913649025
 Normed ED: 0.39501069137562367
 Normed ED: 0.0025044722719141325
 Normed ED: 0.0015180265654648956
 Normed ED: 0.8215552746401338
 Normed ED: 0.625564824469934
 Normed ED: 0.835530257639305
 Normed ED: 0.8686634557495485
 Normed ED: 0.27094105480868663
 Normed ED: 0.07607361963190185
 Normed ED: 0.1411070780399274
 Normed ED: 0.053982883475971036
 Normed ED: 0.15852047556142668
 Normed ED: 0.047873477238726224
 Normed ED: 0.22875816993464052
 Normed ED: 0.0038054968287526427
 Normed ED: 0.00150564617314931
 Normed ED: 0.0015060240963855422
 Normed ED: 0.0016345210853220007
 Normed ED: 0.0036290322580645163
 Normed ED: 0.004118050789293068
 Normed ED: 0.8133638443935927
 Normed ED: 0.4811768794161937
 Normed ED: 0.49471614647333495
 Normed ED: 0.5115830115830116
 Normed ED: 0.22799929738275074
 Normed ED: 0.003219575016097875
 Normed ED: 0.0031783869686134287
 Normed ED: 0.00267538644470868
 Normed ED: 0.00267379679144385
 Normed ED: 0.001201923076923077
 Normed ED: 0.7266926169357412
 Normed ED: 0.34723467862481316
 Normed ED: 0.738307902245379
 Normed ED: 0.7882398906036335
 Normed ED: 0.7214897376648091
 Normed ED: 0.0018917896329928112
 Normed ED: 0.0027808676307007787
 Normed ED: 0.0015180265654648956
 Normed ED: 0.43281098322008044
 Normed ED: 0.27324343506032645
 Normed ED: 0.3032990422135509
 Normed ED: 0.0013489208633093526
 Normed ED: 0.49693251533742333
 Normed ED: 0.09241658240647119
 Normed ED: 0.2674380165289256
 Normed ED: 0.0006793478260869565
 Normed ED: 0.03771661569826707
Pushing model to the hub, epoch 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/sebastian/anaconda3/envs/pt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(
 Normed ED: 0.15269339813689753
 Normed ED: 0.34231997162114225
 Normed ED: 0.0124282982791587
 Normed ED: 0.00205761316872428
 Normed ED: 0.4631319716065479
 Normed ED: 0.12315616951533599
 Normed ED: 0.3805309734513274
 Normed ED: 0.2714574217435207
 Normed ED: 0.17809187279151945
 Normed ED: 0.07308377896613191
 Normed ED: 0.264638783269962
 Normed ED: 0.07020872865275142
 Normed ED: 0.214125788723794
 Normed ED: 0.8284543325526932
 Normed ED: 0.2255677907777013
 Normed ED: 0.0037764350453172208
 Normed ED: 0.3561111111111111
 Normed ED: 0.0006653359946773121
 Normed ED: 0.003682272488164124
 Normed ED: 0.003501945525291829
 Normed ED: 0.3577791918549157
 Normed ED: 0.7957594332512686
 Normed ED: 0.4130262112787927
 Normed ED: 0.0005303632988597189
 Normed ED: 0.574368932038835
 Normed ED: 0.0019743336623889436
 Normed ED: 0.008563273073263558
 Normed ED: 0.008403361344537815
 Normed ED: 0.008466603951081843
 Normed ED: 0.00945179584120983
 Normed ED: 0.5102492429536455
 Normed ED: 0.7106211869107044
 Normed ED: 0.012409513960703205
 Normed ED: 0.07839967670236411
 Normed ED: 0.43453344960757445
 Normed ED: 0.23417489937797292
 Normed ED: 0.2210081071554459
 Normed ED: 0.001924001924001924
 Normed ED: 0.0016863406408094434
 Normed ED: 0.0022404779686333084
 Normed ED: 0.0033407572383073497
 Normed ED: 0.44083840432724813
 Normed ED: 0.004325699745547073
 Normed ED: 0.6266554196541544
 Normed ED: 0.6359223300970874
 Normed ED: 0.004624277456647399
 Normed ED: 0.5720379146919431
 Normed ED: 0.26440566348370104
 Normed ED: 0.1376372586141613
 Normed ED: 0.592999630587366
 Normed ED: 0.652896412486411
 Normed ED: 0.0
 Normed ED: 0.28171701112877584
 Normed ED: 0.2564186965108624
 Normed ED: 0.5709256968889734
 Normed ED: 0.0027447392497712718
 Normed ED: 0.001838235294117647
 Normed ED: 0.0018198362147406734
 Normed ED: 0.0018281535648994515
 Normed ED: 0.0018365472910927456
 Normed ED: 0.0018604651162790699
 Normed ED: 0.001834862385321101
 Normed ED: 0.0018231540565177757
 Normed ED: 0.002737226277372263
 Normed ED: 0.0018535681186283596
 Normed ED: 0.0018365472910927456
 Normed ED: 0.0036663611365719525
 Normed ED: 0.001838235294117647
 Normed ED: 0.0018484288354898336
 Normed ED: 0.0036968576709796672
 Normed ED: 0.0027649769585253456
 Normed ED: 0.001841620626151013
 Normed ED: 0.0018181818181818182
 Normed ED: 0.0018231540565177757
 Normed ED: 0.0018315018315018315
 Normed ED: 0.0018433179723502304
 Normed ED: 0.0027472527472527475
 Normed ED: 0.00818926296633303
 Normed ED: 0.0018365472910927456
 Normed ED: 0.0018433179723502304
 Normed ED: 0.0018501387604070306
 Normed ED: 0.0018315018315018315
 Normed ED: 0.0018365472910927456
 Normed ED: 0.001834862385321101
 Normed ED: 0.003683241252302026
 Normed ED: 0.0027397260273972603
 Normed ED: 0.0018148820326678765
 Normed ED: 0.0018281535648994515
 Normed ED: 0.0018331805682859762
 Normed ED: 0.0018315018315018315
 Normed ED: 0.0018198362147406734
 Normed ED: 0.0018331805682859762
 Normed ED: 0.0036463081130355514
 Normed ED: 0.004604051565377533
 Normed ED: 0.0011148272017837235
 Normed ED: 0.0029190992493744786
 Normed ED: 0.0018812147272238647
 Normed ED: 0.002108433734939759
 Normed ED: 0.0016766467065868263
 Normed ED: 0.08367567567567567
 Normed ED: 0.0033142512805061767
 Normed ED: 0.0029069767441860465
 Normed ED: 0.0018631887143997872
 Normed ED: 0.19735562901030526
 Normed ED: 0.027758174547165372
 Normed ED: 0.20491962037575054
 Normed ED: 0.00667779632721202
 Normed ED: 0.0020073603211776514
 Normed ED: 0.004780114722753346
 Normed ED: 0.003416856492027335
 Normed ED: 0.005309063329541145
 Normed ED: 0.005334170065892689
 Normed ED: 0.039079735266309486
 Normed ED: 0.007544322897019992
 Normed ED: 0.009136735979836169
 Normed ED: 0.004180919802356519
 Normed ED: 0.00778089013383131
 Normed ED: 0.046037735849056606
 Normed ED: 0.004059962523422861
 Normed ED: 0.005040957781978576
 Normed ED: 0.004702194357366771
 Normed ED: 0.003453689167974882
 Normed ED: 0.004805382027871216
 Normed ED: 0.006593406593406593
 Normed ED: 0.009122006841505131
 Normed ED: 0.003867791842475387
 Normed ED: 0.003594249201277955
 Normed ED: 0.6699276226835282
 Normed ED: 0.0046439628482972135
 Normed ED: 0.13317711384495215
 Normed ED: 0.8413897280966768
 Normed ED: 0.5311611633500984
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.1961206896551724
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.00023413720440177945
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.19874551971326165
 Normed ED: 0.0005906674542232723
 Normed ED: 0.000591715976331361
 Normed ED: 0.0010985992859104642
 Normed ED: 0.06677524429967427
 Normed ED: 0.08514088789783093
 Normed ED: 0.07528409090909091
 Normed ED: 0.0
 Normed ED: 0.07887266828872669
 Normed ED: 0.0
 Normed ED: 0.0
 Normed ED: 0.0035530990919857876
 Normed ED: 0.004140215291195142
 Normed ED: 0.004344391785150079
 Normed ED: 0.002771618625277162
 Normed ED: 0.0022093344380005524
 Normed ED: 0.003033645890788748
 Normed ED: 0.0030437188710570003
 Normed ED: 0.003299422601044817
 Normed ED: 0.0019692792437967705
 Normed ED: 0.0027537372147915028
 Normed ED: 0.003581376840429765
 Normed ED: 0.005535566011624689
 Normed ED: 0.003049625727751594
 Normed ED: 0.001190003966679889
 Normed ED: 0.0031583103039873666
 Normed ED: 0.001941747572815534
 Normed ED: 0.0043426766679826295
 Normed ED: 0.0030378348522507597
 Normed ED: 0.004975124378109453
 Normed ED: 0.002730109204368175
 Normed ED: 0.002317497103128621
 Normed ED: 0.003738317757009346
 Normed ED: 0.0027131782945736434
 Normed ED: 0.0025
 Normed ED: 0.006917755572636433
 Normed ED: 0.0011265490048817123
 Normed ED: 0.048537378114842905
 Normed ED: 0.006567425569176883
 Normed ED: 0.0036334275333064193
 Normed ED: 0.4452220726783311
 Normed ED: 0.006804374240583232
 Normed ED: 0.0056942619360490585
 Normed ED: 0.0028388928317956
 Normed ED: 0.5237509779814463
 Normed ED: 0.5689271255060728
 Normed ED: 0.3576781778104335
 Normed ED: 0.0016066838046272494
 Normed ED: 0.5859878835255032
 Normed ED: 0.001903311762466692
 Normed ED: 0.0030807147258163892
 Normed ED: 0.1166563976175806
 Normed ED: 0.004660797514241326
 Normed ED: 0.027761877504293073
 Normed ED: 0.0020460358056265983
 Normed ED: 0.053205699020480855
 Normed ED: 0.005076142131979695
 Normed ED: 0.22847682119205298
 Normed ED: 0.0011876484560570072
 Normed ED: 0.46673464185572267
 Normed ED: 0.15380023875845603
 Normed ED: 0.3697790449037776
 Normed ED: 0.0025044722719141325
 Normed ED: 0.0018968133535660092
 Normed ED: 0.8218814990009379
 Normed ED: 0.625564824469934
 Normed ED: 0.8354179149191132
 Normed ED: 0.8687537627934979
 Normed ED: 0.270596346087556
 Normed ED: 0.1537454164484023
 Normed ED: 0.1397459165154265
 Normed ED: 0.024246395806028834
 Normed ED: 0.02695595003287311
 Normed ED: 0.050865569566146615
 Normed ED: 0.23017134781840665
 Normed ED: 0.006762468300929839
 Normed ED: 0.0017565872020075283
 Normed ED: 0.0015060240963855422
 Normed ED: 0.0013076168682576005
 Normed ED: 0.003227107704719645
 Normed ED: 0.004462753175420529
 Normed ED: 0.8127688787185354
 Normed ED: 0.4808293756515696
 Normed ED: 0.46399606782993363
 Normed ED: 0.5089017589017589
 Normed ED: 0.22571579132267697
 Normed ED: 0.003219575016097875
 Normed ED: 0.0027810885975367503
 Normed ED: 0.00267538644470868
 Normed ED: 0.00267379679144385
 Normed ED: 0.001201923076923077
 Normed ED: 0.7265649926616042
 Normed ED: 0.34723467862481316
 Normed ED: 0.7315469544721499
 Normed ED: 0.7882398906036335
 Normed ED: 0.7195188256082642
 Normed ED: 0.001513431706394249
 Normed ED: 0.0027808676307007787
 Normed ED: 0.0015180265654648956
 Normed ED: 0.431285535986687
 Normed ED: 0.2744854506742371
 Normed ED: 0.27829017382050375
 Normed ED: 0.0013489208633093526
 Normed ED: 0.5195410134060441
 Normed ED: 0.09241658240647119
 Normed ED: 0.2674380165289256
 Normed ED: 0.0384963768115942
 Normed ED: 0.004073319755600814
Pushing model to the hub, epoch 9
`Trainer.fit` stopped: `max_epochs=10` reached.
Pushing model to the hub after training
/home/sebastian/anaconda3/envs/pt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(
{'accuracies': [0.7194513715710723, 0.5732177263969171, 0.9708520179372198, 0.9916666666666667, 0.5590931017848528, 0.7287319422150883, 0.32335329341317365, 0.9386391251518833, 0.829121540312876, 0.921875, 0.5780206435944141, 0.8827586206896552, 0.7169435215946844, 0.17831325301204815, 0.8930180180180181, 0.9906716417910448, 0.7957639939485628, 0.9954022988505747, 0.9798449612403101, 0.983127109111361, 0.891578947368421, 0.35530892827563354, 0.8479284369114878, 0.9891402714932127, 0.6705921938088829, 0.9877551020408163, 0.9800443458980045, 0.9808917197452229, 0.980561555075594, 0.9781181619256017, 0.7563155849203265, 0.5133627701603533, 0.9435626102292769, 0.9840925524222705, 0.7496665184526456, 0.9753787878787878, 0.9722543352601156, 0.9744816586921851, 0.9911971830985915, 0.9887387387387387, 0.9843260188087775, 0.6756509821836455, 0.9849354375896701, 0.6134524141780631, 0.549818587775607, 0.9794628751974723, 0.6768149882903981, 0.8874931731294374, 0.9722405816259088, 0.642791551882461, 0.5160462130937099, 0.9876288659793815, 0.9176795580110497, 0.6693944353518821, 0.6739880765610291, 0.9914772727272727, 0.9942363112391931, 0.994413407821229, 0.9943342776203966, 0.9942528735632183, 0.9940119760479041, 0.994269340974212, 0.9943820224719101, 0.9915492957746479, 0.9940828402366864, 0.9942528735632183, 0.9885714285714285, 0.9913544668587896, 0.9941348973607038, 0.9882697947214076, 0.9912790697674418, 0.9942028985507246, 0.9944289693593314, 0.9943820224719101, 0.9943019943019943, 0.9941860465116279, 0.9914529914529915, 0.9748603351955307, 0.9942528735632183, 0.9941860465116279, 0.9941176470588236, 0.9943019943019943, 0.9942528735632183, 0.994269340974212, 0.9884057971014493, 0.9915014164305949, 0.9944598337950139, 0.9943342776203966, 0.9942857142857143, 0.9943019943019943, 0.994413407821229, 0.9942857142857143, 0.9887640449438202, 0.9855072463768116, 0.9895196506550218, 0.9814585908529048, 0.987331081081081, 0.986351228389445, 0.9886535552193646, 0.9814560439560439, 0.982695810564663, 0.9816849816849816, 0.9877049180327869, 0.98854041013269, 0.9871244635193133, 0.9886769964243146, 0.9698795180722891, 0.9832635983263598, 0.969558599695586, 0.9764267990074442, 0.9703337453646477, 0.9730290456431535, 0.9357894736842105, 0.9634591961023142, 0.9589905362776026, 0.9738480697384807, 0.9645748987854251, 0.9282238442822385, 0.9765066394279878, 0.9726315789473684, 0.9741468459152016, 0.9781021897810219, 0.9691358024691358, 0.9675732217573222, 0.9576587795765878, 0.9786995515695067, 0.9842424242424243, 0.5651819663226507, 0.9791666666666666, 0.9599507085643869, 0.28454439100480955, 0.6934880239520957, 0.9873284054910243, 0.971830985915493, 0.989041095890411, 0.9846153846153847, 0.9893428063943162, 0.9800332778702163, 0.984516129032258, 0.9882988298829883, 0.972027972027972, 0.9846547314578005, 0.9728260869565217, 0.969626168224299, 0.9694117647058823, 0.9832285115303984, 0.9906103286384976, 0.9899923017705927, 0.990726429675425, 0.9892857142857143, 0.9907550077041603, 0.9844961240310077, 0.971830985915493, 0.9725130890052356, 0.9746478873239437, 0.9698952879581152, 0.979047619047619, 0.9811853245531514, 0.9784644194756554, 0.978219696969697, 0.9777158774373259, 0.9779507133592736, 0.9754204398447607, 0.9717741935483871, 0.9696682464454977, 0.9780743565300286, 0.9800796812749004, 0.9738562091503268, 0.9818529130850048, 0.9697766097240473, 0.9783631232361242, 0.9741219963031423, 0.9834815756035579, 0.9852216748768473, 0.9777365491651206, 0.9838107098381071, 0.9871611982881597, 0.982421875, 0.9808673469387755, 0.9863013698630136, 0.9782608695652174, 0.9856985698569857, 0.7103752759381898, 0.9767610748002905, 0.9789603960396039, 0.9875518672199171, 0.7240891404315528, 0.6729539641943734, 0.8011390602752729, 0.9868020304568528, 0.7023926581448705, 0.9919632606199771, 0.9878892733564014, 0.9354430379746835, 0.9841954022988506, 0.9549002601908065, 0.9847328244274809, 0.9838945827232797, 0.9791666666666666, 0.9268707482993197, 0.992018244013683, 0.7389777604369879, 0.9630363036303631, 0.6359269932756965, 0.9883103081827843, 0.9775132275132276, 0.3107057416267942, 0.6454170631144942, 0.30333379064343535, 0.24511858797573083, 0.9411764705882353, 0, 0.9502164502164502, 0.9372881355932203, 0.9299145299145299, 0.9835680751173709, 0.9431891542930924, 0.9717223650385605, 0.989282769991756, 0.9900990099009901, 0.9894736842105263, 0.9870689655172413, 0.9837476099426387, 0.3274512812643855, 0.7555379746835443, 0.7738332668528121, 0.7548269581056466, 0.9918651946542708, 0.9793621013133208, 0.9852774631936579, 0.9795719844357976, 0.9796116504854369, 0.9810606060606061, 0.48626675455943746, 0.9121693121693122, 0.47796830909485566, 0.37441820375797275, 0.49813519813519813, 0.9855769230769231, 0.9789983844911146, 0.9854368932038835, 0.726900318616295, 0.9681712962962963, 0.9468208092485549, 0.9834586466165414, 0.7295245705153816, 0.9922041105598866, 0.8615295480880649, 0.971025841816758, 0.9750778816199377], 'mean_accuracy': 0.9007113127925608} length : 250
Model accuracy dictionary saved to /home/sebastian/Documents/Hauptprojekt/Arrays/DonutInfoExtraction/model_accuracies_after.pkl
{'accuracies': [0.9251870324189526, 0.011560693641618491, 0.899103139013453, 0.725, 0.48335745296671495, 0.8683788121990369, 0.6676646706586826, 0.9264884568651276, 0.829121540312876, 0.65625, 0.395264116575592, 0.7827586206896552, 0.7043189368770764, 0.8650602409638555, 0.9583333333333334, 0.9888059701492538, 0.6444780635400909, 0.9850574712643678, 0.9798449612403101, 0.9820022497187851, 0.638421052631579, 0.3412290872950141, 0.603578154425612, 0.9882352941176471, 0.7170255720053835, 0.986734693877551, 0.9800443458980045, 0.9808917197452229, 0.980561555075594, 0.9781181619256017, 0.7504858142246404, 0.5087148501045782, 0.9647266313932981, 0.9862617498192335, 0.737661182747888, 0.9652777777777778, 0.9855491329479769, 0.9744816586921851, 0.9348591549295775, 0.9786036036036037, 0.9843260188087775, 0.7674737322978529, 0.9842180774748924, 0.6137315099078984, 0.6134524141780631, 0.9684044233807267, 0.704248912679826, 0.983615510649918, 0.9729015201586253, 0.6746250382614019, 0.5848523748395379, 0.9814432989690721, 0.976243093922652, 0.944899072558647, 0.6623784122999686, 0.9914772727272727, 0.9942363112391931, 0.9916201117318436, 0.9915014164305949, 0.9942528735632183, 0.9940119760479041, 0.994269340974212, 0.9943820224719101, 0.9943661971830986, 0.9940828402366864, 0.9942528735632183, 0.9942857142857143, 0.9884726224783862, 0.9941348973607038, 0.9941348973607038, 0.9912790697674418, 0.9942028985507246, 0.9944289693593314, 0.9943820224719101, 0.9943019943019943, 0.9941860465116279, 0.9886039886039886, 0.8463687150837989, 0.9942528735632183, 0.9912790697674418, 0.9911764705882353, 0.9943019943019943, 0.9942528735632183, 0.994269340974212, 0.9884057971014493, 0.9915014164305949, 0.9944598337950139, 0.9943342776203966, 0.9942857142857143, 0.9943019943019943, 0.9916201117318436, 0.9942857142857143, 0.9915730337078652, 0.9797101449275363, 0.9851528384279477, 0.9802224969097652, 0.987331081081081, 0.986351228389445, 0.9886535552193646, 0.9821428571428571, 0.9863387978142076, 0.9804639804639804, 0.9868852459016394, 0.98854041013269, 0.9878397711015737, 0.9874851013110846, 0.9267068273092369, 0.9801255230125523, 0.9756468797564688, 0.9727047146401985, 0.9703337453646477, 0.9730290456431535, 0.9663157894736842, 0.9573690621193667, 0.9631966351209253, 0.9663760896637609, 0.9595141700404858, 0.9635036496350365, 0.975485188968335, 0.9726315789473684, 0.9638055842812823, 0.9760166840458812, 0.9691358024691358, 0.9675732217573222, 0.9663760896637609, 0.9730941704035875, 0.9769696969696969, 0.5353068984247691, 0.90625, 0.9938385705483672, 0.26569608735213834, 0.750748502994012, 0.9873284054910243, 0.971830985915493, 0.9904109589041096, 0.9846153846153847, 0.9893428063943162, 0.9800332778702163, 0.984516129032258, 0.9882988298829883, 0.972027972027972, 0.9846547314578005, 0.9918478260869565, 0.969626168224299, 0.9694117647058823, 0.980083857442348, 0.9906103286384976, 0.9884526558891455, 0.9899536321483772, 0.9892857142857143, 0.9907550077041603, 0.9844961240310077, 0.971830985915493, 0.9790575916230366, 0.9755868544600939, 0.9764397905759162, 0.979047619047619, 0.980244590780809, 0.9756554307116105, 0.9763257575757576, 0.9730733519034355, 0.9753566796368353, 0.9767141009055628, 0.9744623655913979, 0.9791469194312796, 0.9790276453765491, 0.9800796812749004, 0.9764705882352941, 0.9799426934097422, 0.9763469119579501, 0.9764816556914393, 0.9722735674676525, 0.9847522236340533, 0.9852216748768473, 0.9777365491651206, 0.9726027397260274, 0.9871611982881597, 0.984375, 0.9783163265306123, 0.987062404870624, 0.7837528604118993, 0.9273927392739274, 0.6260485651214128, 0.9745824255628177, 0.9801980198019802, 0.9906639004149378, 0.6979129819596745, 0.6349104859335039, 0.6672994779307071, 0.9868020304568528, 0.7381186496230744, 0.9919632606199771, 0.9878892733564014, 0.968987341772152, 0.9841954022988506, 0.98959236773634, 0.9847328244274809, 0.986822840409956, 0.9722222222222222, 0.9897959183673469, 0.992018244013683, 0.7791650409676161, 0.9947194719471947, 0.5965417867435159, 0.9883103081827843, 0.9775132275132276, 0.31040669856459335, 0.6647637170948304, 0.2879681712169022, 0.23960286817429677, 0.9807804309842749, 0.9834630350194552, 0.49350649350649356, 0.9152542372881356, 0.9196581196581196, 0.9788732394366197, 0.9670755326016784, 0.9730077120822622, 0.989282769991756, 0.9900990099009901, 0.9894736842105263, 0.9870689655172413, 0.9866156787762906, 0.2895504066288169, 0.7468354430379747, 0.7786198643797367, 0.6568306010928961, 0.926205694363742, 0.9793621013133208, 0.9852774631936579, 0.9328793774319066, 0.9563106796116505, 0.9810606060606061, 0.44429795649307846, 0.8613756613756614, 0.4362925982200998, 0.3395966212721945, 0.4303030303030303, 0.9819711538461539, 0.9806138933764136, 0.9854368932038835, 0.809285389167046, 0.541087962962963, 0.9838150289017341, 0.9804511278195489, 0.7562924490611267, 0.9886605244507441, 0.9785631517960602, 0.9913860610806577, 0.9672897196261683], 'mean_accuracy': 0.897799170489536} length : 250
Model accuracy dictionary saved to /home/sebastian/Documents/Hauptprojekt/Arrays/DonutInfoExtraction/model_accuracies_after.pkl
/home/sebastian/anaconda3/envs/pt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(
{'accuracies': [0.8890274314214464, 0.5568400770712909, 0.9708520179372198, 0.9916666666666667, 0.543174143753015, 0.680577849117175, 0.5, 0.9580801944106926, 0.9326113116726835, 0.8303571428571428, 0.28051001821493626, 0.8637931034482759, 0.15215946843853823, 0.980722891566265, 0.9847972972972973, 0.9906716417910448, 0.6984367120524457, 0.996551724137931, 0.9813953488372094, 0.984251968503937, 0.9057894736842105, 0.3461984429352327, 0.7796610169491526, 0.9891402714932127, 0.6342530282637955, 0.9775510204081632, 0.975609756097561, 0.9808917197452229, 0.980561555075594, 0.9781181619256017, 0.8021764477263894, 0.5049965140599582, 0.9594356261022927, 0.982646420824295, 0.660738105824811, 0.9627525252525253, 0.9843930635838151, 0.9744816586921851, 0.9911971830985915, 0.9887387387387387, 0.9843260188087775, 0.7555961626313386, 0.9813486370157819, 0.5992185319564611, 0.570750767513257, 0.976303317535545, 0.647039143526263, 0.9628618241398144, 0.9755452742894911, 0.6372819100091827, 0.5183568677792041, 0.979381443298969, 0.919889502762431, 0.9879978177850518, 0.7044242234075934, 0.9914772727272727, 0.9884726224783862, 0.994413407821229, 0.9943342776203966, 0.9942528735632183, 0.9940119760479041, 0.994269340974212, 0.9943820224719101, 0.9943661971830986, 0.9940828402366864, 0.9942528735632183, 0.9885714285714285, 0.9942363112391931, 0.9941348973607038, 0.9941348973607038, 0.9941860465116279, 0.9942028985507246, 0.9944289693593314, 0.9943820224719101, 0.9943019943019943, 0.9941860465116279, 0.9914529914529915, 0.9916201117318436, 0.9913793103448276, 0.9941860465116279, 0.9941176470588236, 0.9857549857549858, 0.9942528735632183, 0.994269340974212, 0.9884057971014493, 0.9943342776203966, 0.9944598337950139, 0.9943342776203966, 0.9942857142857143, 0.9943019943019943, 0.994413407821229, 0.9942857142857143, 0.9915730337078652, 0.9884057971014493, 0.9895196506550218, 0.9814585908529048, 0.987331081081081, 0.986351228389445, 0.9871406959152799, 0.9793956043956044, 0.9799635701275046, 0.9816849816849816, 0.9868852459016394, 0.98854041013269, 0.9878397711015737, 0.9886769964243146, 0.963855421686747, 0.9780334728033473, 0.9528158295281584, 0.9751861042183623, 0.969097651421508, 0.979253112033195, 0.9663157894736842, 0.9610231425091352, 0.9484752891692955, 0.9726027397260274, 0.9767206477732794, 0.9537712895377128, 0.9795709908069459, 0.9821052631578947, 0.9669079627714581, 0.9791449426485923, 0.970679012345679, 0.9633891213389121, 0.9638854296388543, 0.9798206278026906, 0.9745454545454545, 0.6045627376425855, 0.9770833333333333, 0.9938385705483672, 0.28649421552060317, 0.7675898203592815, 0.9873284054910243, 0.971830985915493, 0.989041095890411, 0.9846153846153847, 0.9893428063943162, 0.9800332778702163, 0.984516129032258, 0.9882988298829883, 0.972027972027972, 0.9846547314578005, 0.9918478260869565, 0.969626168224299, 0.971764705882353, 0.9821802935010482, 0.9898278560250391, 0.9876828329484219, 0.990726429675425, 0.9892857142857143, 0.9907550077041603, 0.9844961240310077, 0.971830985915493, 0.9803664921465969, 0.9774647887323944, 0.9764397905759162, 0.979047619047619, 0.9793038570084666, 0.9794007490636704, 0.9791666666666666, 0.9767873723305478, 0.9779507133592736, 0.9715394566623544, 0.9690860215053764, 0.976303317535545, 0.9790276453765491, 0.9800796812749004, 0.9647058823529412, 0.9799426934097422, 0.9776609724047306, 0.9764816556914393, 0.9759704251386322, 0.9847522236340533, 0.9852216748768473, 0.9777365491651206, 0.9813200498132005, 0.9871611982881597, 0.978515625, 0.9783163265306123, 0.9840182648401826, 0.9805491990846682, 0.9856985698569857, 0.6415011037527594, 0.9651416122004357, 0.9876237623762376, 0.991701244813278, 0.7035726918995402, 0.6524936061381075, 0.8054105363075463, 0.9868020304568528, 0.7168141592920354, 0.9919632606199771, 0.9878892733564014, 0.9651898734177216, 0.9827586206896551, 0.9921942758022549, 0.9830364715860899, 0.986822840409956, 0.9826388888888888, 0.9574829931972789, 0.992018244013683, 0.8037456106125634, 0.9900990099009901, 0.9106628242074928, 0.9861849096705633, 0.9788359788359788, 0.2794557416267942, 0.6685696162385031, 0.2591576347921526, 0.2248207391064534, 0.9289458357600466, 0.9562256809338522, 0.9754689754689755, 0.9084745762711864, 0.9179487179487179, 0.9765258215962441, 0.9515816655907037, 0.9781491002570694, 0.9884583676834295, 0.9900990099009901, 0.9894736842105263, 0.9849137931034483, 0.9866156787762906, 0.3183980359060917, 0.7840189873417721, 0.8073394495412844, 0.6743169398907104, 0.9744334689134224, 0.9793621013133208, 0.9830124575311439, 0.9795719844357976, 0.9796116504854369, 0.9810606060606061, 0.4928587123709075, 0.9296296296296296, 0.4896896027783807, 0.38993277021203243, 0.5130536130536131, 0.9819711538461539, 0.9806138933764136, 0.9854368932038835, 0.8771051433773327, 0.9907407407407407, 0.9901734104046243, 0.9834586466165414, 0.7538953256092689, 0.9922041105598866, 0.9803012746234068, 0.9913860610806577, 0.9750778816199377], 'mean_accuracy': 0.9102658056142601} length : 250
Model accuracy dictionary saved to /home/sebastian/Documents/Hauptprojekt/Arrays/DonutInfoExtraction/model_accuracies_after.pkl
Model accuracy dictionary saved to /home/sebastian/Documents/Hauptprojekt/Arrays/DonutInfoExtraction/model_accuracies_after.pkl
/home/sebastian/anaconda3/envs/pt/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(
{'accuracies': [0.7194513715710723, 0.5732177263969171, 0.9708520179372198, 0.9916666666666667, 0.5590931017848528, 0.7287319422150883, 0.32335329341317365, 0.9386391251518833, 0.829121540312876, 0.921875, 0.5780206435944141, 0.8827586206896552, 0.7169435215946844, 0.17831325301204815, 0.8930180180180181, 0.9906716417910448, 0.7957639939485628, 0.9954022988505747, 0.9798449612403101, 0.983127109111361, 0.891578947368421, 0.35530892827563354, 0.8479284369114878, 0.9891402714932127, 0.6705921938088829, 0.9877551020408163, 0.9800443458980045, 0.9808917197452229, 0.980561555075594, 0.9781181619256017, 0.7563155849203265, 0.5133627701603533, 0.9435626102292769, 0.9840925524222705, 0.7496665184526456, 0.9753787878787878, 0.9722543352601156, 0.9744816586921851, 0.9911971830985915, 0.9887387387387387, 0.9843260188087775, 0.6756509821836455, 0.9849354375896701, 0.6134524141780631, 0.549818587775607, 0.9794628751974723, 0.6768149882903981, 0.8874931731294374, 0.9722405816259088, 0.642791551882461, 0.5160462130937099, 0.9876288659793815, 0.9176795580110497, 0.6693944353518821, 0.6739880765610291, 0.9914772727272727, 0.9942363112391931, 0.994413407821229, 0.9943342776203966, 0.9942528735632183, 0.9940119760479041, 0.994269340974212, 0.9943820224719101, 0.9915492957746479, 0.9940828402366864, 0.9942528735632183, 0.9885714285714285, 0.9913544668587896, 0.9941348973607038, 0.9882697947214076, 0.9912790697674418, 0.9942028985507246, 0.9944289693593314, 0.9943820224719101, 0.9943019943019943, 0.9941860465116279, 0.9914529914529915, 0.9748603351955307, 0.9942528735632183, 0.9941860465116279, 0.9941176470588236, 0.9943019943019943, 0.9942528735632183, 0.994269340974212, 0.9884057971014493, 0.9915014164305949, 0.9944598337950139, 0.9943342776203966, 0.9942857142857143, 0.9943019943019943, 0.994413407821229, 0.9942857142857143, 0.9887640449438202, 0.9855072463768116, 0.9895196506550218, 0.9814585908529048, 0.987331081081081, 0.986351228389445, 0.9886535552193646, 0.9814560439560439, 0.982695810564663, 0.9816849816849816, 0.9877049180327869, 0.98854041013269, 0.9871244635193133, 0.9886769964243146, 0.9698795180722891, 0.9832635983263598, 0.969558599695586, 0.9764267990074442, 0.9703337453646477, 0.9730290456431535, 0.9357894736842105, 0.9634591961023142, 0.9589905362776026, 0.9738480697384807, 0.9645748987854251, 0.9282238442822385, 0.9765066394279878, 0.9726315789473684, 0.9741468459152016, 0.9781021897810219, 0.9691358024691358, 0.9675732217573222, 0.9576587795765878, 0.9786995515695067, 0.9842424242424243, 0.5651819663226507, 0.9791666666666666, 0.9599507085643869, 0.28454439100480955, 0.6934880239520957, 0.9873284054910243, 0.971830985915493, 0.989041095890411, 0.9846153846153847, 0.9893428063943162, 0.9800332778702163, 0.984516129032258, 0.9882988298829883, 0.972027972027972, 0.9846547314578005, 0.9728260869565217, 0.969626168224299, 0.9694117647058823, 0.9832285115303984, 0.9906103286384976, 0.9899923017705927, 0.990726429675425, 0.9892857142857143, 0.9907550077041603, 0.9844961240310077, 0.971830985915493, 0.9725130890052356, 0.9746478873239437, 0.9698952879581152, 0.979047619047619, 0.9811853245531514, 0.9784644194756554, 0.978219696969697, 0.9777158774373259, 0.9779507133592736, 0.9754204398447607, 0.9717741935483871, 0.9696682464454977, 0.9780743565300286, 0.9800796812749004, 0.9738562091503268, 0.9818529130850048, 0.9697766097240473, 0.9783631232361242, 0.9741219963031423, 0.9834815756035579, 0.9852216748768473, 0.9777365491651206, 0.9838107098381071, 0.9871611982881597, 0.982421875, 0.9808673469387755, 0.9863013698630136, 0.9782608695652174, 0.9856985698569857, 0.7103752759381898, 0.9767610748002905, 0.9789603960396039, 0.9875518672199171, 0.7240891404315528, 0.6729539641943734, 0.8011390602752729, 0.9868020304568528, 0.7023926581448705, 0.9919632606199771, 0.9878892733564014, 0.9354430379746835, 0.9841954022988506, 0.9549002601908065, 0.9847328244274809, 0.9838945827232797, 0.9791666666666666, 0.9268707482993197, 0.992018244013683, 0.7389777604369879, 0.9630363036303631, 0.6359269932756965, 0.9883103081827843, 0.9775132275132276, 0.3107057416267942, 0.6454170631144942, 0.30333379064343535, 0.24511858797573083, 0.9411764705882353, 0, 0.9502164502164502, 0.9372881355932203, 0.9299145299145299, 0.9835680751173709, 0.9431891542930924, 0.9717223650385605, 0.989282769991756, 0.9900990099009901, 0.9894736842105263, 0.9870689655172413, 0.9837476099426387, 0.3274512812643855, 0.7555379746835443, 0.7738332668528121, 0.7548269581056466, 0.9918651946542708, 0.9793621013133208, 0.9852774631936579, 0.9795719844357976, 0.9796116504854369, 0.9810606060606061, 0.48626675455943746, 0.9121693121693122, 0.47796830909485566, 0.37441820375797275, 0.49813519813519813, 0.9855769230769231, 0.9789983844911146, 0.9854368932038835, 0.726900318616295, 0.9681712962962963, 0.9468208092485549, 0.9834586466165414, 0.7295245705153816, 0.9922041105598866, 0.8615295480880649, 0.971025841816758, 0.9750778816199377], 'mean_accuracy': 0.9007113127925608} length : 250
Model accuracy dictionary saved to /home/sebastian/Documents/Hauptprojekt/Arrays/DonutInfoExtraction/model_accuracies_after.pkl